{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dM4EyO1KSxAN",
        "outputId": "49f4ad2d-2e1f-4afa-8d7b-fe5222846a82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastavro\n",
            "  Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting confluent-kafka\n",
            "  Downloading confluent_kafka-2.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (22 kB)\n",
            "Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading confluent_kafka-2.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fastavro, confluent-kafka\n",
            "Successfully installed confluent-kafka-2.10.0 fastavro-1.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install fastavro confluent-kafka"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0MvrQPoen0y"
      },
      "source": [
        "# Spark Setup\n",
        "\n",
        "Reference: https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WLghayLdt_dm",
        "outputId": "900dcb32-a645-4b12-d424-bde89930c4ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'spark-3.5.5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Fetch the latest Spark 3.x.x version\n",
        "# curl -s https://downloads.apache.org/spark/ → Fetches the Spark download page.\n",
        "# grep -o 'spark-3\\.[0-9]\\+\\.[0-9]\\+' → Extracts only versions that start with spark-3. (ignoring Spark 4.x if it exists in the future).\n",
        "\n",
        "# sort -V → Sorts the versions numerically.\n",
        "# tail -1 → Selects the latest version.\n",
        "spark_version = subprocess.run(\n",
        "    \"curl -s https://downloads.apache.org/spark/ | grep -o 'spark-3\\\\.[0-9]\\\\+\\\\.[0-9]\\\\+' | sort -V | tail -1\",\n",
        "    shell=True, capture_output=True, text=True\n",
        ").stdout.strip()\n",
        "\n",
        "spark_version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUGK1pfcfdQd"
      },
      "outputs": [],
      "source": [
        "spark_release=spark_version\n",
        "hadoop_version='hadoop3'\n",
        "\n",
        "import os, time\n",
        "start=time.time()\n",
        "os.environ['SPARK_RELEASE']=spark_release\n",
        "os.environ['HADOOP_VERSION']=hadoop_version\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_release}-bin-{hadoop_version}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Hm38TuFfdyk",
        "outputId": "5a5a4418-31bf-4cbc-c332-5c9301a70c61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.5\n"
          ]
        }
      ],
      "source": [
        "# Run below commands in google colab\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # install Java8\n",
        "!wget -q http://apache.osuosl.org/spark/${SPARK_RELEASE}/${SPARK_RELEASE}-bin-${HADOOP_VERSION}.tgz # download spark-3.3.X\n",
        "!tar xf ${SPARK_RELEASE}-bin-${HADOOP_VERSION}.tgz # unzip it\n",
        "\n",
        "!pip install -q findspark # install findspark\n",
        "# findspark find your Spark Distribution and sets necessary environment variables\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Check the pyspark version\n",
        "import pyspark\n",
        "print(pyspark.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKK0qqktP3ya"
      },
      "source": [
        "# Define the configuration details for your Spark job:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0Arkhzxg1aZ"
      },
      "source": [
        "Create your Spark session. You must define details of the Kafka Cluster to connect to, topic name and consumer group name.\n",
        "\n",
        "- kafka_brokers: List of Kafka bootstrap servers  \n",
        "- topic_name: The Kafka topic to read messages from\n",
        "- consumer_group: This allows you to use different Spark jobs to consume the same topic messages and implement different analytics\n",
        "- schema: the AVRO schema of topic messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMRPA_3412_M"
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "event_hub_namespace = os.environ.get(\"event_hub_namespace\")\n",
        "\n",
        "passengers_eventhub_name=os.environ.get(\"passengers_eventhub_name\")\n",
        "passengers_conn_str=os.environ.get(\"passengers_conn_str\")\n",
        "\n",
        "drivers_eventhub_name=os.environ.get(\"drivers_eventhub_name\")\n",
        "drivers_conn_str=os.environ.get(\"drivers_conn_str\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBTCdSx3g-Ud"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.avro.functions import from_avro\n",
        "\n",
        "# Define the schema (from github)\n",
        "with open(\"passengerschemav2.json\") as f:\n",
        "    schema = f.read()\n",
        "\n",
        "with open(\"driver_schema.json\") as e:\n",
        "    special_schema = e.read()\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"StreamingAVROFromKafka\") \\\n",
        "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-avro_2.12:3.5.0') \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnuEZtgTgqVG"
      },
      "outputs": [],
      "source": [
        "kafkaConf_rides = {\n",
        "    \"kafka.bootstrap.servers\": f\"{event_hub_namespace}.servicebus.windows.net:9093\",\n",
        "    # Below settins required if kafka is secured, for example when connecting to Azure Event Hubs:\n",
        "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
        "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
        "    \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{passengers_conn_str}\";',\n",
        "\n",
        "    \"subscribe\": passengers_eventhub_name,\n",
        "    \"startingOffsets\": \"latest\", # \"latest\", \"earliest\", (by choosing earliest, you will consume all the data on the event hub immediately)\n",
        "        # by choosing \"latest\", you will consume only newly arriving data.\n",
        "\n",
        "\n",
        "\n",
        "    \"enable.auto.commit\": \"true \",\n",
        "    \"groupIdPrefix\": \"debug_specials_\",\n",
        "    \"auto.commit.interval.ms\": \"5000\"\n",
        "}\n",
        "\n",
        "kafkaConf_specials = {\n",
        "    \"kafka.bootstrap.servers\": f\"{event_hub_namespace}.servicebus.windows.net:9093\",\n",
        "    # Below settins required if kafka is secured, for example when connecting to Azure Event Hubs:\n",
        "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
        "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
        "    \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{drivers_conn_str}\";',\n",
        "\n",
        "    \"subscribe\": drivers_eventhub_name,\n",
        "    \"startingOffsets\": \"latest\", # \"latest\", \"earliest\", (by choosing earliest, you will consume all the data on the event hub immediately)\n",
        "        # by choosing \"latest\", you will consume only newly arriving data.\n",
        "\n",
        "\n",
        "\n",
        "    \"enable.auto.commit\": \"true \",\n",
        "    \"groupIdPrefix\": \"debug_specials_\",\n",
        "    \"auto.commit.interval.ms\": \"5000\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvQ_eIvGTsQe"
      },
      "outputs": [],
      "source": [
        "# Read from Event Hub using Kafka\n",
        "df_rides = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .options(**kafkaConf_rides) \\\n",
        "    .load()\n",
        "\n",
        "# Deserialize the AVRO messages from the value column\n",
        "df_passenger = df_rides.select(from_avro(df_rides.value, schema, {\"mode\": \"PERMISSIVE\"}).alias(\"passenger_events\"))\n",
        "\n",
        "# Read from Event Hub using Kafka\n",
        "df_driver = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .options(**kafkaConf_specials) \\\n",
        "    .load()\n",
        "\n",
        "# Deserialize the AVRO messages from the value column\n",
        "df_driver = df_driver.select(from_avro(df_driver.value, special_schema, {\"mode\": \"PERMISSIVE\"}).alias(\"driver_event\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MzrNKPETLLAg"
      },
      "outputs": [],
      "source": [
        "# Flatten the schemas\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_passenger = df_passenger.select(\n",
        "    col(\"passenger_events.request_id\"),\n",
        "    col(\"passenger_events.passenger_id\"),\n",
        "    col(\"passenger_events.timestamp\"),\n",
        "    col(\"passenger_events.pickup_location.latitude\").alias(\"pickup_latitude\"),\n",
        "    col(\"passenger_events.pickup_location.longitude\").alias(\"pickup_longitude\"),\n",
        "    col(\"passenger_events.dropoff_location.latitude\").alias(\"dropoff_latitude\"),\n",
        "    col(\"passenger_events.dropoff_location.longitude\").alias(\"dropoff_longitude\"),\n",
        "    col(\"passenger_events.vehicle_type\"),\n",
        "    col(\"passenger_events.passenger_preferences.music\").alias(\"music_preference\"),\n",
        "    col(\"passenger_events.passenger_preferences.temperature\").alias(\"preferred_temperature\"),\n",
        "    col(\"passenger_events.passenger_preferences.quiet_ride\").alias(\"quiet_ride\"),\n",
        "    col(\"passenger_events.payment_info.payment_method\").alias(\"payment_method\"),\n",
        "    col(\"passenger_events.payment_info.coupon_codes\").alias(\"coupon_codes\"),\n",
        "    col(\"passenger_events.payment_info.loyalty_points_used\").alias(\"loyalty_points_used\"),\n",
        "    col(\"passenger_events.estimated_fare\"),\n",
        "    col(\"passenger_events.text_messages\"),\n",
        "    col(\"passenger_events.driver_rating\"),\n",
        "    col(\"passenger_events.status\"),\n",
        "    col(\"passenger_events.driver_id\"),\n",
        "    col(\"passenger_events.request_timestamp\"),\n",
        "    col(\"passenger_events.accepted_timestamp\"),\n",
        "    col(\"passenger_events.ride_duration\")\n",
        ")\n",
        "\n",
        "df_driver = df_driver.select(\n",
        "      col(\"driver_event.driver_id\"),\n",
        "      col(\"driver_event.timestamp\"),\n",
        "      col(\"driver_event.latitude\"),\n",
        "      col(\"driver_event.longitude\"),\n",
        "      col(\"driver_event.status\")\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "im0BZXFzjgeS"
      },
      "source": [
        "# Analytical Queries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx18nWfXQVBm"
      },
      "source": [
        "Your Spark job and input messages are ready to be worked on. Now, you can apply any transformations required to answer business questions.\n",
        "\n",
        "IMPORTANT NOTE: if in config you chose \"startingOffsets\": \"latest\", then you must send data AFTER running df.writeStream...\n",
        "In other words, Spark will only start 'consuming' events after you run .writeStream, meaning that it will show up as empty if no new events have been sent after running .writeStream. (For this to be a real-time analytics case, it should be set to latest, so our stats update as new data comes in. For testing purposes, easier to set it to 'earliest' cause then you just send once and can work with that)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNJMd8DRCc9T"
      },
      "source": [
        "## Setup of Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEkMmi234eGt"
      },
      "outputs": [],
      "source": [
        "!mkdir checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If offset:Latest, send new events after running this cell.\n",
        "query_name='all_passengers'\n",
        "query_passengers=df_passenger.writeStream \\\n",
        "    .outputMode(\"update\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(query_name) \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "q_B1yG4LmFEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If offset:Latest, send new events after running this cell.\n",
        "query_name='all_drivers'\n",
        "query_drivers=df_driver.writeStream \\\n",
        "    .outputMode(\"update\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(query_name) \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "Eva8uy4ypRzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('show tables').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6x93DNtkpgI0",
        "outputId": "a6fdfa2f-97ca-4eb7-89f0-3de53cfec2c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------+-----------+\n",
            "|namespace|     tableName|isTemporary|\n",
            "+---------+--------------+-----------+\n",
            "|         |   all_drivers|       true|\n",
            "|         |all_passengers|       true|\n",
            "+---------+--------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "active_queries = spark.streams.active\n",
        "\n",
        "# Print details about each active query\n",
        "for query in active_queries:\n",
        "    print(f\"Query Name: {query.name}\")\n",
        "    print(f\"Query ID: {query.id}\")\n",
        "    print(f\"Query Status: {query.status}\")\n",
        "    print(f\"Is Query Active: {query.isActive}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "claFYP-NExez",
        "outputId": "15a7066a-e51f-49a9-fdb7-625a7e3563d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query Name: status_counts\n",
            "Query ID: 5b9c87df-d4d7-4151-b849-c3bf40c3fb04\n",
            "Query Status: {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n",
            "Is Query Active: True\n",
            "--------------------------------------------------\n",
            "Query Name: all_passengers\n",
            "Query ID: 82a391d0-397b-4f52-aff5-22a743ab33a9\n",
            "Query Status: {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n",
            "Is Query Active: True\n",
            "--------------------------------------------------\n",
            "Query Name: ride_duration_sql\n",
            "Query ID: 002a295a-265d-4ac4-8ec1-1a2d58f63651\n",
            "Query Status: {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n",
            "Is Query Active: True\n",
            "--------------------------------------------------\n",
            "Query Name: response_time_sql4\n",
            "Query ID: 2399ebe6-dfde-4ddd-982f-d5660e69f436\n",
            "Query Status: {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n",
            "Is Query Active: True\n",
            "--------------------------------------------------\n",
            "Query Name: driver_metrics_sql\n",
            "Query ID: f9d08ccb-210d-45ab-84b7-101418e7550f\n",
            "Query Status: {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n",
            "Is Query Active: True\n",
            "--------------------------------------------------\n",
            "Query Name: all_drivers\n",
            "Query ID: 61ac0892-ce27-4f30-9901-b77aa577d13e\n",
            "Query Status: {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n",
            "Is Query Active: True\n",
            "--------------------------------------------------\n",
            "Query Name: demand_supply_sql\n",
            "Query ID: 4a731c54-92e4-4be4-a8b7-d0c9546f8313\n",
            "Query Status: {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n",
            "Is Query Active: True\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Tm86HWwUVw1",
        "outputId": "afafa0a2-5f95-4d3a-98de-47ab3eadc36e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'message': 'Processing new data',\n",
              " 'isDataAvailable': True,\n",
              " 'isTriggerActive': True}"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "# Status either \"Processing new data\" or \"Getting offsets from...\"\n",
        "query_passengers.status"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_drivers.status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAg7r3CVpn93",
        "outputId": "e5028312-5d26-4bbd-a154-c893ef4cbbcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'message': 'Processing new data',\n",
              " 'isDataAvailable': True,\n",
              " 'isTriggerActive': True}"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark.sql(f'SELECT count(*) as record_count FROM all_drivers').show(20, truncate=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZ-kpix2kw1O",
        "outputId": "d25b44ec-0c2e-4913-c8ee-0c34707d3c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|record_count|\n",
            "+------------+\n",
            "|      116030|\n",
            "+------------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark.sql(f'SELECT count(*) as record_count FROM all_passengers').show(20, truncate=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gVCG9rsq0Ty",
        "outputId": "1e7de232-0924-41c6-aea4-068b412d1bf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|record_count|\n",
            "+------------+\n",
            "|         535|\n",
            "+------------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TubCWjP0Dlb",
        "outputId": "06bab8b6-5e10-4a26-ddf3-289bd618bca8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+------------+-------------+------------------+------------------+------------------+------------------+------------+----------------+---------------------+----------+--------------+------------+-------------------+--------------+--------------------+-------------+-----------+----------+-----------------+------------------+-------------+\n",
            "|         request_id|passenger_id|    timestamp|   pickup_latitude|  pickup_longitude|  dropoff_latitude| dropoff_longitude|vehicle_type|music_preference|preferred_temperature|quiet_ride|payment_method|coupon_codes|loyalty_points_used|estimated_fare|       text_messages|driver_rating|     status| driver_id|request_timestamp|accepted_timestamp|ride_duration|\n",
            "+-------------------+------------+-------------+------------------+------------------+------------------+------------------+------------+----------------+---------------------+----------+--------------+------------+-------------------+--------------+--------------------+-------------+-----------+----------+-----------------+------------------+-------------+\n",
            "|REQ-1745433870-5919|      P00580|1745433870776|42.202535152975116|-73.73371900526614| 39.33286468226331|-74.83612497733465|      LUXURY|             POP|                   19|      true|    DEBIT_CARD|          []|               NULL|      500.1123|                  []|          2.0|  COMPLETED|driver_286|    1745433870776|     1745464900533|    39808.984|\n",
            "|REQ-1745434588-7681|      P00306|1745434588021|40.823559149223044|-73.88179527255517| 40.79887568635326| -73.9033731132494|     ECONOMY|   NO_PREFERENCE|                   20|     false|        PAYPAL|          []|               NULL|      20.63314|                  []|         NULL|  COMPLETED|driver_119|    1745434588021|     1745434660601|    394.23447|\n",
            "|REQ-1745434593-1745|      P00444|1745434593060| 40.70990070721034|-73.97672482758705| 40.69589162609156|-73.93588894743102|     ECONOMY|            ROCK|                   26|     false|   CREDIT_CARD|          []|               NULL|      8.157659|                  []|          1.9|  COMPLETED|driver_294|    1745434593060|     1745434769551|     452.6127|\n",
            "|REQ-1745434590-6353|      P00332|1745434590036| 40.65163882204543| -73.9550313300839|40.630428390018636|-74.06174310695687|     ECONOMY|   NO_PREFERENCE|                   23|     false|     APPLE_PAY|          []|               NULL|     16.436935|                  []|         NULL|  CANCELLED|driver_213|    1745434590036|     1745434652512|    352.81882|\n",
            "|REQ-1745434595-6738|      P00095|1745434595072| 40.64253762981028| -74.0062182751312|40.657140871253645|-73.97429152700818|     ECONOMY|            JAZZ|                   25|     false|    GOOGLE_PAY|          []|               NULL|     7.2093534|                  []|          1.4|  COMPLETED|driver_241|    1745434595072|     1745434953714|    376.74826|\n",
            "|REQ-1745434595-4556|      P00134|1745434595072| 40.66298268207481|-74.01024535647939| 40.66751227911622|-74.05545275073858|     ECONOMY|            JAZZ|                   19|      true|   CREDIT_CARD|          []|               NULL|      8.259072|                  []|          3.2|  COMPLETED|driver_082|    1745434595072|     1745434963790|    460.72577|\n",
            "|REQ-1745434590-6777|      P00135|1745434590035| 40.68775422439305|-73.94439415892236| 40.61275621640599|-73.93586595287412|     ECONOMY|   NO_PREFERENCE|                   22|      true|          CASH|          []|               NULL|      81.84935|                  []|         NULL|  CANCELLED|driver_107|    1745434590035|     1745434900127|    452.94318|\n",
            "|REQ-1745434589-5926|      P00549|1745434589029| 40.70468764457563|-74.03792759459698| 40.60887771558895|-73.97146354076936|     ECONOMY|            ROCK|                   19|      true|     APPLE_PAY|          []|               NULL|     20.526543|                  []|         NULL|  COMPLETED|driver_155|    1745434589029|     1745434739614|    1442.1234|\n",
            "|REQ-1745434588-5015|      P00009|1745434588023| 40.89131231931289|-73.87438830358397| 40.74623757884058|-73.89439053929942|     ECONOMY|       CLASSICAL|                   23|     false|          CASH|          []|               NULL|     145.83531|[{SYS-1745434588-...|         NULL|  CANCELLED|driver_219|    1745434588023|     1745434703757|    743.64996|\n",
            "|REQ-1745434592-6518|      P00457|1745434592050| 40.82827847475717|-74.12123165921614|  40.7467049643736|-74.11578853487036|      LUXURY|            ROCK|                   21|      true|     APPLE_PAY|          []|                 16|     16.099312|                  []|          2.5|  COMPLETED|driver_214|    1745434592050|     1745435206769|     1087.945|\n",
            "|REQ-1745434594-4194|      P00060|1745434594066|40.812379899994035|-74.04486716233988| 40.69278949248748|-74.05701494406178|     ECONOMY|            JAZZ|                   24|      true|    DEBIT_CARD|          []|               NULL|     22.470667|                  []|          3.8|IN_PROGRESS|driver_124|    1745434594066|     1745434702607|     142.5226|\n",
            "|REQ-1745434589-4096|      P00205|1745434589028|40.747365884813505|-74.05177880677255| 40.80957880433938|-73.84989991446196|     ECONOMY|       CLASSICAL|                   22|     false|    DEBIT_CARD|    [SAVE44]|               NULL|     163.22467|                  []|          1.8|  CANCELLED|driver_200|    1745434589028|     1745434642678|    367.21957|\n",
            "|REQ-1745434593-4305|      P00432|1745434593060| 40.88020177599314|-74.00608813548476| 40.74138875400309|-73.94081607249944|         SUV|             POP|                   24|     false|          CASH|          []|               NULL|     27.032446|                  []|         NULL|  COMPLETED|driver_255|    1745434593060|     1745434826763|    1962.5957|\n",
            "|REQ-1745434590-8863|      P00453|1745434590035|40.851698359803414|-73.88367527438862| 40.65288431619948|-74.00529985574143|     ECONOMY|            JAZZ|                   26|     false|          CASH|          []|               NULL|     38.984383|                  []|         NULL|  COMPLETED|driver_018|    1745434590035|     1745434764243|    2918.7505|\n",
            "|REQ-1745434593-8837|      P00074|1745434593059| 40.77216222094563|-73.89229838939146| 40.70990263447101|-74.13615234211295|     ECONOMY|   NO_PREFERENCE|                   18|      true|    GOOGLE_PAY|          []|                 76|      34.96219|                  []|          3.1|  COMPLETED|driver_157|    1745434593059|     1745434794558|     2596.975|\n",
            "|REQ-1745434591-9092|      P00331|1745434591041| 40.84046775033981|-73.85253374687005| 40.64923322613395|-74.03123486403747|     ECONOMY|       CLASSICAL|                   20|      true|    DEBIT_CARD|          []|               NULL|     226.01202|                  []|         NULL|  COMPLETED|driver_118|    1745434591041|     1745434842280|     3120.993|\n",
            "|REQ-1745434588-4629|      P00585|1745434588022|  40.8880697331782|-73.93994299053192| 40.62483934326531|-73.92368643794461|     ECONOMY|             POP|                   26|     false|   CREDIT_CARD|    [SAVE28]|               NULL|     252.49042|                  []|         NULL|  COMPLETED|driver_099|    1745434588022|     1745434728394|    3510.0632|\n",
            "|REQ-1745434614-5253|      P00356|1745434614098| 40.77781013328819|-74.03024429975638|40.781300269585834|-74.13137663004457|     ECONOMY|         HIP_HOP|                   26|      true|   CREDIT_CARD|          []|               NULL|     27.135727|                  []|         NULL|  COMPLETED|driver_218|    1745434614098|     1745434701690|    1021.1077|\n",
            "|REQ-1745434594-5950|      P00310|1745434594066| 40.89159069705073|-74.00894675793641| 40.65894604166474|-73.96350897479812|     ECONOMY|       CLASSICAL|                   22|     false|    GOOGLE_PAY|          []|               NULL|      74.05639|                  []|         NULL|  COMPLETED|driver_278|    1745434594066|     1745434800124|    3132.5376|\n",
            "|REQ-1745434617-4749|      P00175|1745434617130| 40.80169806793381|-74.10670520408357|40.744814288800235|-74.07597429386004|         SUV|   NO_PREFERENCE|                   25|     false|          CASH|    [SAVE44]|               NULL|     12.733148|                  []|          4.0|  COMPLETED|driver_193|    1745434617130|     1745435305326|    818.65186|\n",
            "+-------------------+------------+-------------+------------------+------------------+------------------+------------------+------------+----------------+---------------------+----------+--------------+------------+-------------------+--------------+--------------------+-------------+-----------+----------+-----------------+------------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(spark.sql(f'SELECT * FROM all_passengers').show(20, truncate=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark.sql(f'SELECT * FROM all_drivers').show(20, truncate=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdD95QSoogr0",
        "outputId": "ff29c27a-000c-4093-e605-6a141aace52c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+------------------+------------------+---------+\n",
            "| driver_id|    timestamp|          latitude|         longitude|   status|\n",
            "+----------+-------------+------------------+------------------+---------+\n",
            "|driver_286|1745434579166| 39.33286468226331|-74.83612497733465|AVAILABLE|\n",
            "|driver_003|1745434588018|  40.7473101765056|-74.00916149075474|  OFFLINE|\n",
            "|driver_004|1745434588018| 40.76182214782082|-73.87144994657183|AVAILABLE|\n",
            "|driver_006|1745434588018| 40.78862051110513| -74.0016071273915|AVAILABLE|\n",
            "|driver_007|1745434588018|  40.6011054271701|-74.07100029862085|AVAILABLE|\n",
            "|driver_010|1745434588018| 40.64571990296338|-73.85426034232256|  OFFLINE|\n",
            "|driver_013|1745434588018|40.695090755677185|-73.99588073073915|AVAILABLE|\n",
            "|driver_018|1745434588018| 40.86255608741879|-73.87403509187507|AVAILABLE|\n",
            "|driver_019|1745434588018| 40.68342131565475|-73.89520028184695|AVAILABLE|\n",
            "|driver_020|1745434588018|  40.8298137586657|-74.02561874207238|  OFFLINE|\n",
            "|driver_022|1745434588018| 40.68830393701036|-74.10235882437439|AVAILABLE|\n",
            "|driver_023|1745434588018| 40.86601731978925|-74.02671862950385|AVAILABLE|\n",
            "|driver_026|1745434588018|40.860149493826775|-74.05353564444171|  OFFLINE|\n",
            "|driver_029|1745434588018| 40.63179786575676|-73.93573008084908|AVAILABLE|\n",
            "|driver_031|1745434588018|40.681436881823835|-73.88932988514811|AVAILABLE|\n",
            "|driver_032|1745434588018|40.633310147793686|-73.97889238307192|AVAILABLE|\n",
            "|driver_033|1745434588018| 40.73079995368281|-74.04777445212584|  OFFLINE|\n",
            "|driver_034|1745434588018|40.639709307808495|-73.92333794768874|AVAILABLE|\n",
            "|driver_035|1745434588018| 40.70175689777151|-73.93459521671654|AVAILABLE|\n",
            "|driver_036|1745434588018| 40.68395058558354|-73.86878003059101|  OFFLINE|\n",
            "+----------+-------------+------------------+------------------+---------+\n",
            "only showing top 20 rows\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Github"
      ],
      "metadata": {
        "id": "viIgIorsEX0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write df_passenger to CSV\n",
        "query_passengers_to_csv = df_passenger.writeStream \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"path\", \"/tmp/passengers_stream_output\") \\\n",
        "    .option(\"checkpointLocation\", \"/tmp/passengers_checkpoint\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .start()\n",
        "\n",
        "# Write df_driver to CSV\n",
        "query_drivers_to_csv = df_driver.writeStream \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"path\", \"/tmp/drivers_stream_output\") \\\n",
        "    .option(\"checkpointLocation\", \"/tmp/drivers_checkpoint\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .start()\n",
        "\n",
        "time.sleep(35) # PRODUCER SHOULD BE SEIDNING NOW!!!\n",
        "\n",
        "query_passengers_to_csv.stop()\n",
        "query_drivers_to_csv.stop()\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "\n",
        "# Collect from memory sink and drop unsupported columns\n",
        "drivers_df = spark.sql(\"SELECT * FROM all_drivers\")\n",
        "passengers_df = spark.sql(\"SELECT * FROM all_passengers\").drop(\"coupon_codes\", \"text_messages\")\n",
        "\n",
        "# Coalesce to 1 partition and write with header\n",
        "drivers_df.coalesce(1).write.option(\"header\", True).mode(\"overwrite\").csv(\"/tmp/final_drivers_single\")\n",
        "passengers_df.coalesce(1).write.option(\"header\", True).mode(\"overwrite\").csv(\"/tmp/final_passengers_single\")\n",
        "\n",
        "# Rename single part file for drivers\n",
        "driver_csv = glob.glob(\"/tmp/final_drivers_single/part-*.csv\")[0]\n",
        "os.rename(driver_csv, \"/tmp/final_drivers_single/drivers.csv\")\n",
        "\n",
        "# Rename single part file for passengers\n",
        "passenger_csv = glob.glob(\"/tmp/final_passengers_single/part-*.csv\")[0]\n",
        "os.rename(passenger_csv, \"/tmp/final_passengers_single/passengers.csv\")\n",
        "\n",
        "# Zip folders\n",
        "shutil.make_archive(\"/tmp/final_drivers_csv_single\", 'zip', \"/tmp/final_drivers_single\")\n",
        "shutil.make_archive(\"/tmp/final_passengers_csv_single\", 'zip', \"/tmp/final_passengers_single\")\n",
        "\n",
        "# Move for download\n",
        "!cp /tmp/final_drivers_csv_single.zip /content/final_drivers_csv_single.zip\n",
        "!cp /tmp/final_passengers_csv_single.zip /content/final_passengers_csv_single.zip"
      ],
      "metadata": {
        "id": "xHlZS6hl_yNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import os\n",
        "from pyspark.sql.functions import col\n",
        "import getpass\n",
        "\n",
        "# Define repository details\n",
        "colleagues_username = \"esanchezmex\"\n",
        "repo_name = \"RideHailDataGen\"\n",
        "\n",
        "# Get your GitHub Personal Access Token securely\n",
        "print(\"Enter your GitHub Personal Access Token (it won't be displayed):\")\n",
        "personal_access_token = getpass.getpass()\n",
        "\n",
        "# Repository URL with embedded credentials\n",
        "authenticated_url = f\"https://Markibariki1:{personal_access_token}@github.com/{colleagues_username}/{repo_name}.git\"\n",
        "\n",
        "# Configure git\n",
        "!git config --global user.name \"Markibariki1\"\n",
        "!git config --global user.email \"mhaupter.ieu2021@student.ie.edu\"\n",
        "\n",
        "# Create a new temporary directory (clean previous attempts)\n",
        "!rm -rf /tmp/github_export\n",
        "!mkdir -p /tmp/github_export\n",
        "\n",
        "try:\n",
        "    # Clone the repository with authentication\n",
        "    !git clone {authenticated_url} /tmp/github_export\n",
        "\n",
        "    # For drivers - handle the nested structure properly\n",
        "    drivers_df = spark.sql(\"SELECT * FROM all_drivers\")\n",
        "    flattened_drivers = drivers_df.select(\n",
        "        col(\"driver_event.driver_id\"),\n",
        "        col(\"driver_event.timestamp\"),\n",
        "        col(\"driver_event.latitude\"),\n",
        "        col(\"driver_event.longitude\"),\n",
        "        col(\"driver_event.status\")\n",
        "    )\n",
        "\n",
        "    # For passengers - drop unsupported columns\n",
        "    passengers_df = spark.sql(\"SELECT * FROM all_passengers\").drop(\"coupon_codes\", \"text_messages\")\n",
        "\n",
        "    # Save the dataframes as CSV files\n",
        "    flattened_drivers.coalesce(1).write.option(\"header\", True).mode(\"overwrite\").csv(\"/tmp/github_export/drivers_temp\")\n",
        "    passengers_df.coalesce(1).write.option(\"header\", True).mode(\"overwrite\").csv(\"/tmp/github_export/passengers_temp\")\n",
        "\n",
        "    # Rename the part files\n",
        "    !find /tmp/github_export/drivers_temp -name \"part-*.csv\" -exec mv {} /tmp/github_export/drivers.csv \\;\n",
        "    !find /tmp/github_export/passengers_temp -name \"part-*.csv\" -exec mv {} /tmp/github_export/passengers.csv \\;\n",
        "\n",
        "    # Clean up\n",
        "    !rm -rf /tmp/github_export/drivers_temp /tmp/github_export/passengers_temp\n",
        "\n",
        "    # Update README\n",
        "    with open(\"/tmp/github_export/README.md\", \"w\") as f:\n",
        "        f.write(\"# Ride Hailing Data Export\\n\\n\")\n",
        "        f.write(\"This repository contains data exported from a ride-hailing simulation.\\n\\n\")\n",
        "        f.write(\"## Files\\n\\n\")\n",
        "        f.write(\"- `drivers.csv`: Driver location and status data\\n\")\n",
        "        f.write(\"- `passengers.csv`: Passenger ride request data\\n\\n\")\n",
        "        f.write(f\"Last updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    # Commit and push changes with embedded credentials\n",
        "    !cd /tmp/github_export && git add .\n",
        "    !cd /tmp/github_export && git commit -m \"Update ride-hailing data\"\n",
        "    !cd /tmp/github_export && git push {authenticated_url}\n",
        "\n",
        "    print(f\"✅ Data successfully pushed to GitHub: https://github.com/{colleagues_username}/{repo_name}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "cbeO3_-VEn3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 1: number of driver updates per status"
      ],
      "metadata": {
        "id": "gK0syWHFsEmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import from_unixtime, col\n",
        "\n",
        "df_driver_ts = df_driver.withColumn(\n",
        "    \"timestamp_ts\",\n",
        "    from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ")\n",
        "\n",
        "df_driver_ts.createOrReplaceTempView(\"driver_events_view\")"
      ],
      "metadata": {
        "id": "ILdwlR0eufa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ride_metrics_query = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        'ride_metrics' AS metric_type,\n",
        "        status AS dimension,\n",
        "        COUNT(*) AS metric_value,\n",
        "        window(timestamp_ts, '1 hour') AS window_interval\n",
        "    FROM driver_events_view\n",
        "    GROUP BY window(timestamp_ts, '1 hour'), status\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "FaZB_FTIt7UX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ride_metrics_query.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"driver_metrics_sql\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "ivokk23hu8S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM driver_metrics_sql\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3c2wRZjubP8",
        "outputId": "1be7b09e-0612-4134-ac95-82e7876b967b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+---------+------------+------------------------------------------+\n",
            "|metric_type |dimension|metric_value|window_interval                           |\n",
            "+------------+---------+------------+------------------------------------------+\n",
            "|ride_metrics|ON_RIDE  |45351       |{2025-04-23 19:00:00, 2025-04-23 20:00:00}|\n",
            "|ride_metrics|AVAILABLE|119317      |{2025-04-23 19:00:00, 2025-04-23 20:00:00}|\n",
            "|ride_metrics|OFFLINE  |394802      |{2025-04-23 19:00:00, 2025-04-23 20:00:00}|\n",
            "+------------+---------+------------+------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 2: Trips status"
      ],
      "metadata": {
        "id": "CGXcBf1gDV_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_passenger.groupBy(\"status\").count() \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"status_counts\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRtNgf1o80Ou",
        "outputId": "f6d9c056-af32-4bc2-c902-561b28350ae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.query.StreamingQuery at 0x7c0a66e9dbd0>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM status_counts\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga3xVX4I84ZR",
        "outputId": "76eece83-52e7-4aea-f47d-63b37d49caea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----+\n",
            "|     status|count|\n",
            "+-----------+-----+\n",
            "|  CANCELLED|   21|\n",
            "|IN_PROGRESS|   10|\n",
            "|  COMPLETED|  356|\n",
            "+-----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 3: average response time per vehicle type"
      ],
      "metadata": {
        "id": "A-gfHUcSv30i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, from_unixtime, window, unix_timestamp\n",
        "\n",
        "df_passenger_ts = df_passenger.withColumn(\n",
        "    \"timestamp_ts\", from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"request_ts\", from_unixtime(col(\"request_timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"accepted_ts\", from_unixtime(col(\"accepted_timestamp\") / 1000).cast(\"timestamp\")\n",
        ")\n",
        "\n",
        "df_passenger_ts.createOrReplaceTempView(\"passenger_events_view\")"
      ],
      "metadata": {
        "id": "j7bQTbS07wq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_time_query = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        'response_time' AS metric_type,\n",
        "        vehicle_type AS dimension,\n",
        "        AVG((unix_timestamp(accepted_ts) - unix_timestamp(request_ts)) / 60.0) AS metric_value,\n",
        "        window(timestamp_ts, '15 minutes', '5 minutes').end AS window_end\n",
        "    FROM passenger_events_view\n",
        "    WHERE status = 'COMPLETED'\n",
        "    GROUP BY window(timestamp_ts, '15 minutes', '5 minutes'), vehicle_type\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "zRdBbKuC78vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_time_query.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"response_time_sql4\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "se9WKNZu7-XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM response_time_sql4\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3suVkCx47_0o",
        "outputId": "79937e94-4a9e-4563-a43f-c46d4e648790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+---------+-------------+-------------------+\n",
            "|metric_type  |dimension|metric_value |window_end         |\n",
            "+-------------+---------+-------------+-------------------+\n",
            "|response_time|POOL     |35.2111111667|2025-04-23 19:30:00|\n",
            "|response_time|SUV      |14.0258332500|2025-04-23 19:10:00|\n",
            "|response_time|LUXURY   |17.5228394815|2025-04-23 19:15:00|\n",
            "|response_time|ECONOMY  |7.3405833350 |2025-04-23 19:15:00|\n",
            "|response_time|SUV      |13.6265431852|2025-04-23 19:20:00|\n",
            "|response_time|SUV      |13.6265431852|2025-04-23 19:25:00|\n",
            "|response_time|ECONOMY  |5.5629186603 |2025-04-23 19:20:00|\n",
            "|response_time|POOL     |20.3666670000|2025-04-23 19:05:00|\n",
            "|response_time|LUXURY   |24.0999999200|2025-04-23 19:30:00|\n",
            "|response_time|LUXURY   |23.2608973462|2025-04-23 19:20:00|\n",
            "|response_time|POOL     |32.9285714286|2025-04-23 19:25:00|\n",
            "|response_time|POOL     |19.8000000000|2025-04-23 19:15:00|\n",
            "|response_time|SUV      |13.5682539048|2025-04-23 19:15:00|\n",
            "|response_time|LUXURY   |18.1089743462|2025-04-23 19:05:00|\n",
            "|response_time|ECONOMY  |5.5629186603 |2025-04-23 19:25:00|\n",
            "|response_time|POOL     |20.3666670000|2025-04-23 19:10:00|\n",
            "|response_time|LUXURY   |23.2608973462|2025-04-23 19:25:00|\n",
            "|response_time|LUXURY   |18.1089743462|2025-04-23 19:10:00|\n",
            "|response_time|SUV      |14.0258332500|2025-04-23 19:05:00|\n",
            "|response_time|ECONOMY  |5.8200000053 |2025-04-23 19:30:00|\n",
            "+-------------+---------+-------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 4: average ride duration per hour of day"
      ],
      "metadata": {
        "id": "lAXHtOJUDM8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import from_unixtime, col\n",
        "\n",
        "df_passenger_ts = df_passenger.withColumn(\n",
        "    \"timestamp_ts\", from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"request_ts\", from_unixtime(col(\"request_timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"accepted_ts\", from_unixtime(col(\"accepted_timestamp\") / 1000).cast(\"timestamp\")\n",
        ")\n",
        "\n",
        "df_passenger_ts.createOrReplaceTempView(\"passenger_events_view\")"
      ],
      "metadata": {
        "id": "itY7f42jopMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import hour\n",
        "\n",
        "ride_duration_query = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        'ride_duration' AS metric_type,\n",
        "        CAST(HOUR(timestamp_ts) AS STRING) AS dimension,\n",
        "        AVG(ride_duration) AS metric_value,\n",
        "        window(timestamp_ts, '60 minutes', '60 minutes').end AS window_end\n",
        "    FROM passenger_events_view\n",
        "    WHERE status = 'COMPLETED'\n",
        "    GROUP BY window(timestamp_ts, '60 minutes', '60 minutes'), HOUR(timestamp_ts)\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "IuTm6dznoMGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ride_duration_query.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"ride_duration_sql\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-ci-HSWoWUB",
        "outputId": "8e03726d-96a0-45e7-d6b1-8ca05b86ee02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.query.StreamingQuery at 0x7c0a66eb0210>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM ride_duration_sql\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_Tsi1BJoYqm",
        "outputId": "9cc39824-16cc-4f9a-ee80-00150d4cf781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+---------+------------------+-------------------+\n",
            "|metric_type  |dimension|metric_value      |window_end         |\n",
            "+-------------+---------+------------------+-------------------+\n",
            "|ride_duration|19       |2066.1144578331537|2025-04-23 20:00:00|\n",
            "|ride_duration|20       |1784.512392572996 |2025-04-23 21:00:00|\n",
            "+-------------+---------+------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 5: requested/accepted ratio per area"
      ],
      "metadata": {
        "id": "SHAfBQ_XpPcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, from_unixtime, when\n",
        "\n",
        "df_passenger_quadrant = df_passenger.withColumn(\n",
        "    \"timestamp\", from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"quadrant\", when((col(\"pickup_latitude\") >= 40.75) & (col(\"pickup_longitude\") >= -73.95), \"NE\")\n",
        "                .when((col(\"pickup_latitude\") >= 40.75) & (col(\"pickup_longitude\") < -73.95), \"NW\")\n",
        "                .when((col(\"pickup_latitude\") < 40.75) & (col(\"pickup_longitude\") >= -73.95), \"SE\")\n",
        "                .otherwise(\"SW\")\n",
        ")\n",
        "\n",
        "df_passenger_quadrant.createOrReplaceTempView(\"passenger_quadrant_view\")"
      ],
      "metadata": {
        "id": "_Bx89eMoJE_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ride_ratio_query = spark.sql(\"\"\"\n",
        "SELECT\n",
        "  'request_acceptance_ratio' AS metric_type,\n",
        "  quadrant AS dimension,\n",
        "  SUM(CASE WHEN accepted_timestamp IS NOT NULL THEN 1 ELSE 0 END) * 1.0 /\n",
        "  COUNT(*) AS metric_value,\n",
        "  window(timestamp, '15 minutes', '5 minutes') AS window_interval\n",
        "FROM passenger_quadrant_view\n",
        "GROUP BY window(timestamp, '15 minutes', '5 minutes'), quadrant\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "_WN6IUNGJJBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ride_ratio_query.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"ride_ratio_sql\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dp-mMJUrJLrl",
        "outputId": "6483afe1-2597-451e-d117-d1ce316b1784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.query.StreamingQuery at 0x7c0a5c3ec890>"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM ride_ratio_sql\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjhRu5lLJMd1",
        "outputId": "f2190fbe-1c15-43f4-a04a-81fc06665955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------+---------+------------------+------------------------------------------+\n",
            "|metric_type             |dimension|metric_value      |window_interval                           |\n",
            "+------------------------+---------+------------------+------------------------------------------+\n",
            "|request_acceptance_ratio|SW       |1.0000000000000000|{2025-04-23 19:25:00, 2025-04-23 19:40:00}|\n",
            "|request_acceptance_ratio|SE       |1.0000000000000000|{2025-04-23 19:30:00, 2025-04-23 19:45:00}|\n",
            "|request_acceptance_ratio|NW       |1.0000000000000000|{2025-04-23 19:50:00, 2025-04-23 20:05:00}|\n",
            "|request_acceptance_ratio|NW       |1.0000000000000000|{2025-04-23 19:20:00, 2025-04-23 19:35:00}|\n",
            "|request_acceptance_ratio|SW       |1.0000000000000000|{2025-04-23 19:40:00, 2025-04-23 19:55:00}|\n",
            "|request_acceptance_ratio|SW       |1.0000000000000000|{2025-04-23 20:05:00, 2025-04-23 20:20:00}|\n",
            "|request_acceptance_ratio|NE       |1.0000000000000000|{2025-04-23 19:25:00, 2025-04-23 19:40:00}|\n",
            "|request_acceptance_ratio|SE       |1.0000000000000000|{2025-04-23 19:35:00, 2025-04-23 19:50:00}|\n",
            "|request_acceptance_ratio|SE       |1.0000000000000000|{2025-04-23 19:50:00, 2025-04-23 20:05:00}|\n",
            "|request_acceptance_ratio|NE       |1.0000000000000000|{2025-04-23 19:55:00, 2025-04-23 20:10:00}|\n",
            "|request_acceptance_ratio|NE       |1.0000000000000000|{2025-04-23 20:00:00, 2025-04-23 20:15:00}|\n",
            "|request_acceptance_ratio|NW       |1.0000000000000000|{2025-04-23 19:40:00, 2025-04-23 19:55:00}|\n",
            "|request_acceptance_ratio|SW       |1.0000000000000000|{2025-04-23 19:30:00, 2025-04-23 19:45:00}|\n",
            "|request_acceptance_ratio|NW       |1.0000000000000000|{2025-04-23 19:55:00, 2025-04-23 20:10:00}|\n",
            "|request_acceptance_ratio|SW       |1.0000000000000000|{2025-04-23 19:35:00, 2025-04-23 19:50:00}|\n",
            "|request_acceptance_ratio|NE       |1.0000000000000000|{2025-04-23 19:40:00, 2025-04-23 19:55:00}|\n",
            "|request_acceptance_ratio|NW       |1.0000000000000000|{2025-04-23 20:00:00, 2025-04-23 20:15:00}|\n",
            "|request_acceptance_ratio|SE       |1.0000000000000000|{2025-04-23 19:55:00, 2025-04-23 20:10:00}|\n",
            "|request_acceptance_ratio|NW       |1.0000000000000000|{2025-04-23 19:25:00, 2025-04-23 19:40:00}|\n",
            "|request_acceptance_ratio|NW       |1.0000000000000000|{2025-04-23 19:45:00, 2025-04-23 20:00:00}|\n",
            "+------------------------+---------+------------------+------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 6: average response time per vehicle type"
      ],
      "metadata": {
        "id": "JjJ8T835-MV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, from_unixtime\n",
        "\n",
        "df_passenger_ts = df_passenger.withColumn(\n",
        "    \"timestamp\", from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"request_ts\", from_unixtime(col(\"request_timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"accepted_ts\", from_unixtime(col(\"accepted_timestamp\") / 1000).cast(\"timestamp\")\n",
        ")\n",
        "\n",
        "df_passenger_ts.createOrReplaceTempView(\"passenger_events_view\")"
      ],
      "metadata": {
        "id": "KEF7TgsAHiL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_time_query = spark.sql(\"\"\"\n",
        "SELECT\n",
        "  'response_time' AS metric_type,\n",
        "  vehicle_type AS dimension,\n",
        "  AVG((unix_timestamp(accepted_ts) - unix_timestamp(request_ts)) / 60.0) AS metric_value,\n",
        "  window(timestamp, '15 minutes', '5 minutes') AS window_interval\n",
        "FROM passenger_events_view\n",
        "WHERE status = 'COMPLETED'\n",
        "GROUP BY window(timestamp, '15 minutes', '5 minutes'), vehicle_type\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "FflJbSE1Hsly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_time_query.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"response_time_sql\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKZ-g-uxHvVn",
        "outputId": "920a247a-cf0a-4754-9b8f-fc2fae16d061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.query.StreamingQuery at 0x7c0a5c3a0410>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM response_time_sql\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pjInaLbHxGB",
        "outputId": "c0191277-7129-4645-a9f2-62343411d2f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+---------+-------------+------------------------------------------+\n",
            "|metric_type  |dimension|metric_value |window_interval                           |\n",
            "+-------------+---------+-------------+------------------------------------------+\n",
            "|response_time|LUXURY   |16.4388890000|{2025-04-23 19:15:00, 2025-04-23 19:30:00}|\n",
            "|response_time|ECONOMY  |2.4843750000 |{2025-04-23 19:25:00, 2025-04-23 19:40:00}|\n",
            "|response_time|SUV      |4.8250000000 |{2025-04-23 19:25:00, 2025-04-23 19:40:00}|\n",
            "|response_time|LUXURY   |16.4388890000|{2025-04-23 19:25:00, 2025-04-23 19:40:00}|\n",
            "|response_time|ECONOMY  |2.4843750000 |{2025-04-23 19:15:00, 2025-04-23 19:30:00}|\n",
            "|response_time|SUV      |4.8250000000 |{2025-04-23 19:15:00, 2025-04-23 19:30:00}|\n",
            "|response_time|SUV      |4.8250000000 |{2025-04-23 19:20:00, 2025-04-23 19:35:00}|\n",
            "|response_time|ECONOMY  |2.4843750000 |{2025-04-23 19:20:00, 2025-04-23 19:35:00}|\n",
            "|response_time|LUXURY   |16.4388890000|{2025-04-23 19:20:00, 2025-04-23 19:35:00}|\n",
            "+-------------+---------+-------------+------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 7: Active drivers per area"
      ],
      "metadata": {
        "id": "1mLaNSBFH66d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quadrants:\n",
        "\n",
        "NE: lat ≥ 40.75 and lon ≥ -73.95\n",
        "\n",
        "NW: lat ≥ 40.75 and lon < -73.95\n",
        "\n",
        "SE: lat < 40.75 and lon ≥ -73.95\n",
        "\n",
        "SW: lat < 40.75 and lon < -73.95\n",
        "\n"
      ],
      "metadata": {
        "id": "TRYCsFuXImlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "df_driver_quadrant = df_driver.withColumn(\n",
        "    \"timestamp\", from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"quadrant\", when((col(\"latitude\") >= 40.75) & (col(\"longitude\") >= -73.95), \"NE\")\n",
        "                .when((col(\"latitude\") >= 40.75) & (col(\"longitude\") < -73.95), \"NW\")\n",
        "                .when((col(\"latitude\") < 40.75) & (col(\"longitude\") >= -73.95), \"SE\")\n",
        "                .otherwise(\"SW\")\n",
        ")\n",
        "\n",
        "df_driver_quadrant.createOrReplaceTempView(\"driver_quadrant_view\")"
      ],
      "metadata": {
        "id": "SBi-d5L8Ing6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "online_drivers_query = spark.sql(\"\"\"\n",
        "SELECT\n",
        "  'online_driver_count' AS metric_type,\n",
        "  quadrant AS dimension,\n",
        "  COUNT(*) AS metric_value,\n",
        "  window(timestamp, '15 minutes', '5 minutes') AS window_interval\n",
        "FROM driver_quadrant_view\n",
        "WHERE status IN ('AVAILABLE', 'ON_RIDE')\n",
        "GROUP BY window(timestamp, '15 minutes', '5 minutes'), quadrant\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "ueoH4L8sIqXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "online_drivers_query.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"online_drivers_sql\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpC8K-gDIr0_",
        "outputId": "87bfc6f4-d193-4b52-8428-39a6ac78b52c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.query.StreamingQuery at 0x7c0a5c3ec8d0>"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM online_drivers_sql\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaP0WcNpIuJf",
        "outputId": "09ec2a4e-08c5-48da-acb2-3794a53cc11f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+---------+------------+------------------------------------------+\n",
            "|metric_type        |dimension|metric_value|window_interval                           |\n",
            "+-------------------+---------+------------+------------------------------------------+\n",
            "|online_driver_count|SW       |477         |{2025-04-23 19:25:00, 2025-04-23 19:40:00}|\n",
            "|online_driver_count|NE       |337         |{2025-04-23 19:25:00, 2025-04-23 19:40:00}|\n",
            "|online_driver_count|SE       |322         |{2025-04-23 19:30:00, 2025-04-23 19:45:00}|\n",
            "|online_driver_count|NW       |476         |{2025-04-23 19:20:00, 2025-04-23 19:35:00}|\n",
            "|online_driver_count|SW       |477         |{2025-04-23 19:30:00, 2025-04-23 19:45:00}|\n",
            "|online_driver_count|NW       |476         |{2025-04-23 19:25:00, 2025-04-23 19:40:00}|\n",
            "|online_driver_count|NE       |337         |{2025-04-23 19:30:00, 2025-04-23 19:45:00}|\n",
            "|online_driver_count|NW       |476         |{2025-04-23 19:30:00, 2025-04-23 19:45:00}|\n",
            "|online_driver_count|SE       |322         |{2025-04-23 19:25:00, 2025-04-23 19:40:00}|\n",
            "|online_driver_count|NE       |337         |{2025-04-23 19:20:00, 2025-04-23 19:35:00}|\n",
            "|online_driver_count|SW       |477         |{2025-04-23 19:20:00, 2025-04-23 19:35:00}|\n",
            "|online_driver_count|SE       |322         |{2025-04-23 19:20:00, 2025-04-23 19:35:00}|\n",
            "+-------------------+---------+------------+------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 8: Average Wait Time per Area"
      ],
      "metadata": {
        "id": "RT3H0iOCJnEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_wait_ts = df_passenger.withColumn(\n",
        "    \"timestamp\", from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"request_ts\", from_unixtime(col(\"request_timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"accepted_ts\", from_unixtime(col(\"accepted_timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"quadrant\", when((col(\"pickup_latitude\") >= 40.75) & (col(\"pickup_longitude\") >= -73.95), \"NE\")\n",
        "                .when((col(\"pickup_latitude\") >= 40.75) & (col(\"pickup_longitude\") < -73.95), \"NW\")\n",
        "                .when((col(\"pickup_latitude\") < 40.75) & (col(\"pickup_longitude\") >= -73.95), \"SE\")\n",
        "                .otherwise(\"SW\")\n",
        ")\n",
        "\n",
        "df_wait_ts.createOrReplaceTempView(\"wait_time_quadrant_view\")"
      ],
      "metadata": {
        "id": "ZF3tz_3OJ_SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wait_time_query = spark.sql(\"\"\"\n",
        "SELECT\n",
        "  'average_wait_time' AS metric_type,\n",
        "  quadrant AS dimension,\n",
        "  AVG(unix_timestamp(accepted_ts) - unix_timestamp(request_ts)) AS metric_value,\n",
        "  window(timestamp, '15 minutes', '5 minutes') AS window_interval\n",
        "FROM wait_time_quadrant_view\n",
        "WHERE accepted_ts IS NOT NULL AND request_ts IS NOT NULL\n",
        "GROUP BY window(timestamp, '15 minutes', '5 minutes'), quadrant\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "dlOz4f5WLiCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wait_time_query.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"wait_time_sql\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OihV__WJL4Bl",
        "outputId": "7f451ace-bdce-497c-ad50-0c55afb83102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.query.StreamingQuery at 0x7c0a5c27f690>"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM wait_time_sql\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYfuYcBcL6bA",
        "outputId": "fdb9430f-c447-4c80-9174-bc431f4c04e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+---------+------------------+------------------------------------------+\n",
            "|metric_type      |dimension|metric_value      |window_interval                           |\n",
            "+-----------------+---------+------------------+------------------------------------------+\n",
            "|average_wait_time|SW       |429.7294117647059 |{2025-04-23 19:40:00, 2025-04-23 19:55:00}|\n",
            "|average_wait_time|SE       |500.62857142857143|{2025-04-23 19:35:00, 2025-04-23 19:50:00}|\n",
            "|average_wait_time|NW       |484.4769230769231 |{2025-04-23 19:40:00, 2025-04-23 19:55:00}|\n",
            "|average_wait_time|SW       |429.7294117647059 |{2025-04-23 19:35:00, 2025-04-23 19:50:00}|\n",
            "|average_wait_time|SE       |500.62857142857143|{2025-04-23 19:40:00, 2025-04-23 19:55:00}|\n",
            "|average_wait_time|NE       |430.7560975609756 |{2025-04-23 19:40:00, 2025-04-23 19:55:00}|\n",
            "|average_wait_time|NW       |402.390625        |{2025-04-23 19:45:00, 2025-04-23 20:00:00}|\n",
            "|average_wait_time|NW       |5738.0            |{2025-04-23 19:30:00, 2025-04-23 19:45:00}|\n",
            "|average_wait_time|NE       |430.7560975609756 |{2025-04-23 19:35:00, 2025-04-23 19:50:00}|\n",
            "|average_wait_time|NW       |484.4769230769231 |{2025-04-23 19:35:00, 2025-04-23 19:50:00}|\n",
            "|average_wait_time|SE       |500.62857142857143|{2025-04-23 19:45:00, 2025-04-23 20:00:00}|\n",
            "|average_wait_time|SW       |429.7294117647059 |{2025-04-23 19:45:00, 2025-04-23 20:00:00}|\n",
            "|average_wait_time|NE       |430.7560975609756 |{2025-04-23 19:45:00, 2025-04-23 20:00:00}|\n",
            "+-----------------+---------+------------------+------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 9: Driver Utilization Rate"
      ],
      "metadata": {
        "id": "xcTu-ALPL92K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_driver_ts = df_driver.withColumn(\n",
        "    \"timestamp\", from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ")\n",
        "\n",
        "df_driver_ts.createOrReplaceTempView(\"driver_events_view\")"
      ],
      "metadata": {
        "id": "FQ54uo38MJZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "utilization_query = spark.sql(\"\"\"\n",
        "SELECT\n",
        "  'driver_utilization' AS metric_type,\n",
        "  'global' AS dimension,\n",
        "  SUM(CASE WHEN status = 'ON_RIDE' THEN 1 ELSE 0 END) * 1.0 /\n",
        "  SUM(CASE WHEN status IN ('AVAILABLE', 'ON_RIDE') THEN 1 ELSE 0 END) AS metric_value,\n",
        "  window(timestamp, '15 minutes', '5 minutes') AS window_interval\n",
        "FROM driver_events_view\n",
        "WHERE status IN ('AVAILABLE', 'ON_RIDE')\n",
        "GROUP BY window(timestamp, '15 minutes', '5 minutes')\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "jtq48YfhMO8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "utilization_query.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"utilization_sql\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEm84UTuMSzm",
        "outputId": "17d64629-0343-4944-f5c8-9d8f0f9e1529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.query.StreamingQuery at 0x7c0a5c418a10>"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM utilization_sql\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCyPxQWGMVe8",
        "outputId": "2ea12780-c7cf-4c8d-e5aa-0e07404cf07a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+---------+------------------+------------------------------------------+\n",
            "|metric_type       |dimension|metric_value      |window_interval                           |\n",
            "+------------------+---------+------------------+------------------------------------------+\n",
            "|driver_utilization|global   |0.2710934764850041|{2025-04-23 19:40:00, 2025-04-23 19:55:00}|\n",
            "|driver_utilization|global   |0.2710934764850041|{2025-04-23 19:45:00, 2025-04-23 20:00:00}|\n",
            "|driver_utilization|global   |0.3380406001765225|{2025-04-23 19:50:00, 2025-04-23 20:05:00}|\n",
            "|driver_utilization|global   |0.2470252260828177|{2025-04-23 19:35:00, 2025-04-23 19:50:00}|\n",
            "+------------------+---------+------------------+------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 10: Vehicle Type Demand Share"
      ],
      "metadata": {
        "id": "FZK3lM-IMXL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_passenger_ts = df_passenger.withColumn(\n",
        "    \"timestamp\", from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ")\n",
        "\n",
        "df_passenger_ts.createOrReplaceTempView(\"passenger_events_view\")"
      ],
      "metadata": {
        "id": "ncc_sJKmMkj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vehicle_counts_query = spark.sql(\"\"\"\n",
        "SELECT\n",
        "  vehicle_type,\n",
        "  window(timestamp, '15 minutes', '5 minutes') AS window_interval,\n",
        "  COUNT(*) AS vehicle_count\n",
        "FROM passenger_events_view\n",
        "WHERE status = 'COMPLETED'\n",
        "GROUP BY window(timestamp, '15 minutes', '5 minutes'), vehicle_type\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "9h9gnFnNNJJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_requests_query = spark.sql(\"\"\"\n",
        "SELECT\n",
        "  window(timestamp, '15 minutes', '5 minutes') AS window_interval,\n",
        "  COUNT(*) AS total_count\n",
        "FROM passenger_events_view\n",
        "WHERE status = 'COMPLETED'\n",
        "GROUP BY window(timestamp, '15 minutes', '5 minutes')\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "HIm_q2tTNkBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vehicle_counts_query.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"vehicle_counts_sql\") \\\n",
        "    .start()\n",
        "\n",
        "total_requests_query.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"total_requests_sql\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FnUxMrdNn_A",
        "outputId": "6df9d139-2596-4852-ac04-cd1df69cfd86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.query.StreamingQuery at 0x7c0a5c3fc250>"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT\n",
        "  'vehicle_type_share' AS metric_type,\n",
        "  vc.vehicle_type AS dimension,\n",
        "  vc.window_interval,\n",
        "  vc.vehicle_count * 1.0 / tr.total_count AS metric_value\n",
        "FROM vehicle_counts_sql vc\n",
        "JOIN total_requests_sql tr\n",
        "ON vc.window_interval = tr.window_interval\n",
        "\"\"\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-BlnwH-NxRG",
        "outputId": "be225ce1-fd70-4a9f-f504-a37aa162a56a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+---------+------------------------------------------+-------------------+\n",
            "|metric_type       |dimension|window_interval                           |metric_value       |\n",
            "+------------------+---------+------------------------------------------+-------------------+\n",
            "|vehicle_type_share|LUXURY   |{2025-04-23 19:40:00, 2025-04-23 19:55:00}|2.0000000000000000 |\n",
            "|vehicle_type_share|SUV      |{2025-04-23 19:45:00, 2025-04-23 20:00:00}|1.5000000000000000 |\n",
            "|vehicle_type_share|ECONOMY  |{2025-04-23 19:45:00, 2025-04-23 20:00:00}|10.5000000000000000|\n",
            "|vehicle_type_share|ECONOMY  |{2025-04-23 19:40:00, 2025-04-23 19:55:00}|10.5000000000000000|\n",
            "|vehicle_type_share|LUXURY   |{2025-04-23 19:50:00, 2025-04-23 20:05:00}|2.0000000000000000 |\n",
            "|vehicle_type_share|LUXURY   |{2025-04-23 19:45:00, 2025-04-23 20:00:00}|2.0000000000000000 |\n",
            "|vehicle_type_share|SUV      |{2025-04-23 19:40:00, 2025-04-23 19:55:00}|1.5000000000000000 |\n",
            "|vehicle_type_share|ECONOMY  |{2025-04-23 19:50:00, 2025-04-23 20:05:00}|10.5000000000000000|\n",
            "|vehicle_type_share|SUV      |{2025-04-23 19:50:00, 2025-04-23 20:05:00}|1.5000000000000000 |\n",
            "+------------------+---------+------------------------------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 11: Cancellation Rate"
      ],
      "metadata": {
        "id": "PjZghRIyNzC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cancellation_query = spark.sql(\"\"\"\n",
        "SELECT\n",
        "  'cancellation_rate' AS metric_type,\n",
        "  'global' AS dimension,\n",
        "  window(timestamp, '15 minutes', '5 minutes') AS window_interval,\n",
        "  SUM(CASE WHEN status = 'CANCELLED' THEN 1 ELSE 0 END) * 1.0 /\n",
        "  COUNT(*) AS metric_value\n",
        "FROM passenger_events_view\n",
        "GROUP BY window(timestamp, '15 minutes', '5 minutes')\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "yFXBurFVOSTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cancellation_query.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"cancellation_rate_sql\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTVyPHF2OX7o",
        "outputId": "44aa93c5-fb92-4df9-84c5-90cd52b28a1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.query.StreamingQuery at 0x7c0a5d058850>"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM cancellation_rate_sql\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjeVge5WOZrY",
        "outputId": "29ddda04-62a8-4b94-ae0d-5f5e2be18e67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+---------+------------------------------------------+------------------+\n",
            "|metric_type      |dimension|window_interval                           |metric_value      |\n",
            "+-----------------+---------+------------------------------------------+------------------+\n",
            "|cancellation_rate|global   |{2025-04-23 19:50:00, 2025-04-23 20:05:00}|0.0588235294117647|\n",
            "|cancellation_rate|global   |{2025-04-23 19:45:00, 2025-04-23 20:00:00}|0.0594059405940594|\n",
            "|cancellation_rate|global   |{2025-04-23 20:00:00, 2025-04-23 20:15:00}|0.0000000000000000|\n",
            "|cancellation_rate|global   |{2025-04-23 19:55:00, 2025-04-23 20:10:00}|0.0588235294117647|\n",
            "+-----------------+---------+------------------------------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 12: Ride Matching Delay by Area"
      ],
      "metadata": {
        "id": "Wt8TfKUxOeL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_match_ts = df_passenger.withColumn(\n",
        "    \"timestamp\", from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"request_ts\", from_unixtime(col(\"request_timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"accepted_ts\", from_unixtime(col(\"accepted_timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"quadrant\", when((col(\"pickup_latitude\") >= 40.75) & (col(\"pickup_longitude\") >= -73.95), \"NE\")\n",
        "                .when((col(\"pickup_latitude\") >= 40.75) & (col(\"pickup_longitude\") < -73.95), \"NW\")\n",
        "                .when((col(\"pickup_latitude\") < 40.75) & (col(\"pickup_longitude\") >= -73.95), \"SE\")\n",
        "                .otherwise(\"SW\")\n",
        ")\n",
        "\n",
        "df_match_ts.createOrReplaceTempView(\"matching_delay_view\")"
      ],
      "metadata": {
        "id": "GXIyAT4PQMLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "match_delay_query = spark.sql(\"\"\"\n",
        "SELECT\n",
        "  'match_delay' AS metric_type,\n",
        "  quadrant AS dimension,\n",
        "  AVG((unix_timestamp(accepted_ts) - unix_timestamp(request_ts)) / 60.0) AS metric_value,\n",
        "  window(timestamp, '15 minutes', '5 minutes') AS window_interval\n",
        "FROM matching_delay_view\n",
        "WHERE accepted_ts IS NOT NULL AND request_ts IS NOT NULL\n",
        "GROUP BY window(timestamp, '15 minutes', '5 minutes'), quadrant\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "VvdggUy9Qx_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "match_delay_query.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"match_delay_sql\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QNY4fQ6QzvS",
        "outputId": "0017436c-d246-4be6-ad9e-6d4b590d8250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.query.StreamingQuery at 0x7c0a5c419690>"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM match_delay_sql\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ayndo4TIQ1zL",
        "outputId": "1c0a6148-5743-40f9-e8b8-8ba29441142f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+------------+------------------------------------------+\n",
            "|metric_type|dimension|metric_value|window_interval                           |\n",
            "+-----------+---------+------------+------------------------------------------+\n",
            "|match_delay|NE       |8.0217948974|{2025-04-23 20:05:00, 2025-04-23 20:20:00}|\n",
            "|match_delay|SW       |7.4877193509|{2025-04-23 20:05:00, 2025-04-23 20:20:00}|\n",
            "|match_delay|NE       |8.0217948974|{2025-04-23 20:00:00, 2025-04-23 20:15:00}|\n",
            "|match_delay|NW       |7.1575758000|{2025-04-23 19:55:00, 2025-04-23 20:10:00}|\n",
            "|match_delay|NE       |8.0217948974|{2025-04-23 19:55:00, 2025-04-23 20:10:00}|\n",
            "|match_delay|SE       |6.5232142857|{2025-04-23 20:05:00, 2025-04-23 20:20:00}|\n",
            "|match_delay|SW       |7.4877193509|{2025-04-23 20:00:00, 2025-04-23 20:15:00}|\n",
            "|match_delay|SW       |7.4877193509|{2025-04-23 19:55:00, 2025-04-23 20:10:00}|\n",
            "|match_delay|NW       |7.1575758000|{2025-04-23 20:00:00, 2025-04-23 20:15:00}|\n",
            "|match_delay|SE       |6.5232142857|{2025-04-23 19:55:00, 2025-04-23 20:10:00}|\n",
            "|match_delay|NW       |7.1575758000|{2025-04-23 20:05:00, 2025-04-23 20:20:00}|\n",
            "|match_delay|SE       |6.5232142857|{2025-04-23 20:00:00, 2025-04-23 20:15:00}|\n",
            "+-----------+---------+------------+------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 13: Drive Efficiency (high-level)"
      ],
      "metadata": {
        "id": "kiWiCQhlRCtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Measures the distance and sees if drivers are being efficient**"
      ],
      "metadata": {
        "id": "d5NZRRn_TulF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Efficiency Ratio= Ride Duration / Straight-line Distance\n",
        "​"
      ],
      "metadata": {
        "id": "vrgVVpWrRFmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We assume Haversine distance (straight-line) is “ideal”\n",
        "\n",
        "Duration = ride_duration in seconds"
      ],
      "metadata": {
        "id": "dtNVKJZXRssW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, radians, sin, cos, atan2, sqrt, lit\n",
        "\n",
        "R = 6371.0  # Earth radius in kilometers\n",
        "\n",
        "df_efficiency = df_passenger.withColumn(\n",
        "    \"pickup_lat_rad\", radians(col(\"pickup_latitude\"))\n",
        ").withColumn(\n",
        "    \"pickup_lon_rad\", radians(col(\"pickup_longitude\"))\n",
        ").withColumn(\n",
        "    \"dropoff_lat_rad\", radians(col(\"dropoff_latitude\"))\n",
        ").withColumn(\n",
        "    \"dropoff_lon_rad\", radians(col(\"dropoff_longitude\"))\n",
        ").withColumn(\n",
        "    \"dlat\", col(\"dropoff_lat_rad\") - col(\"pickup_lat_rad\")\n",
        ").withColumn(\n",
        "    \"dlon\", col(\"dropoff_lon_rad\") - col(\"pickup_lon_rad\")\n",
        ").withColumn(\n",
        "    \"a\", sin(col(\"dlat\") / 2) ** 2 + cos(col(\"pickup_lat_rad\")) * cos(col(\"dropoff_lat_rad\")) * sin(col(\"dlon\") / 2) ** 2\n",
        ").withColumn(\n",
        "    \"c\", 2 * atan2(sqrt(col(\"a\")), sqrt(1 - col(\"a\")))\n",
        ").withColumn(\n",
        "    \"distance_km\", R * col(\"c\")\n",
        ").withColumn(\n",
        "    \"timestamp\", from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ")\n",
        "\n",
        "df_efficiency.createOrReplaceTempView(\"route_efficiency_view\")"
      ],
      "metadata": {
        "id": "UG8lpHS0RlJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "efficiency_query = spark.sql(\"\"\"\n",
        "SELECT\n",
        "  'route_efficiency' AS metric_type,\n",
        "  vehicle_type AS dimension,\n",
        "  AVG(distance_km / (ride_duration / 60.0)) AS metric_value,  -- km per minute\n",
        "  window(timestamp, '15 minutes', '5 minutes') AS window_interval\n",
        "FROM route_efficiency_view\n",
        "WHERE ride_duration > 0 AND distance_km IS NOT NULL\n",
        "GROUP BY window(timestamp, '15 minutes', '5 minutes'), vehicle_type\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "egxD3XP8SPLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "efficiency_query.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"route_efficiency_sql\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0l9eKSsWSRv6",
        "outputId": "9b643079-b701-4a4d-a89d-de617e8dfcb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.query.StreamingQuery at 0x7c0a5c3edd50>"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM route_efficiency_sql\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNZWRIg1SVKN",
        "outputId": "29f8d430-1451-4892-eb84-8e8c71f48e13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+---------+-------------------+------------------------------------------+\n",
            "|metric_type     |dimension|metric_value       |window_interval                           |\n",
            "+----------------+---------+-------------------+------------------------------------------+\n",
            "|route_efficiency|ECONOMY  |0.586264382388866  |{2025-04-23 20:10:00, 2025-04-23 20:25:00}|\n",
            "|route_efficiency|LUXURY   |0.5008780255137506 |{2025-04-23 20:10:00, 2025-04-23 20:25:00}|\n",
            "|route_efficiency|SUV      |0.5008777970136895 |{2025-04-23 20:05:00, 2025-04-23 20:20:00}|\n",
            "|route_efficiency|POOL     |0.32765227232195027|{2025-04-23 20:10:00, 2025-04-23 20:25:00}|\n",
            "|route_efficiency|POOL     |0.32765227232195027|{2025-04-23 20:00:00, 2025-04-23 20:15:00}|\n",
            "|route_efficiency|SUV      |0.5008777970136895 |{2025-04-23 20:10:00, 2025-04-23 20:25:00}|\n",
            "|route_efficiency|LUXURY   |0.5008780255137506 |{2025-04-23 20:00:00, 2025-04-23 20:15:00}|\n",
            "|route_efficiency|POOL     |0.32765227232195027|{2025-04-23 20:05:00, 2025-04-23 20:20:00}|\n",
            "|route_efficiency|ECONOMY  |0.586264382388866  |{2025-04-23 20:00:00, 2025-04-23 20:15:00}|\n",
            "|route_efficiency|SUV      |0.5008777970136895 |{2025-04-23 20:00:00, 2025-04-23 20:15:00}|\n",
            "|route_efficiency|ECONOMY  |0.586264382388866  |{2025-04-23 20:05:00, 2025-04-23 20:20:00}|\n",
            "|route_efficiency|LUXURY   |0.5008780255137506 |{2025-04-23 20:05:00, 2025-04-23 20:20:00}|\n",
            "+----------------+---------+-------------------+------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "vRGfCXt5SWoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 14: Pricing Analytics Anomalies (High-level)"
      ],
      "metadata": {
        "id": "S2SiRZFET7ZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, radians, sin, cos, atan2, sqrt, lit, when, unix_timestamp, window\n",
        "\n",
        "R = 6371.0  # Earth radius in km\n",
        "\n",
        "df_anomaly = df_passenger.withColumn(\"pickup_lat_rad\", radians(col(\"pickup_latitude\"))) \\\n",
        "    .withColumn(\"pickup_lon_rad\", radians(col(\"pickup_longitude\"))) \\\n",
        "    .withColumn(\"dropoff_lat_rad\", radians(col(\"dropoff_latitude\"))) \\\n",
        "    .withColumn(\"dropoff_lon_rad\", radians(col(\"dropoff_longitude\"))) \\\n",
        "    .withColumn(\"dlat\", col(\"dropoff_lat_rad\") - col(\"pickup_lat_rad\")) \\\n",
        "    .withColumn(\"dlon\", col(\"dropoff_lon_rad\") - col(\"pickup_lon_rad\")) \\\n",
        "    .withColumn(\"a\", sin(col(\"dlat\") / 2) ** 2 + cos(col(\"pickup_lat_rad\")) * cos(col(\"dropoff_lat_rad\")) * sin(col(\"dlon\") / 2) ** 2) \\\n",
        "    .withColumn(\"c\", 2 * atan2(sqrt(col(\"a\")), sqrt(1 - col(\"a\")))) \\\n",
        "    .withColumn(\"distance_km\", R * col(\"c\")) \\\n",
        "    .withColumn(\"duration_min\", col(\"ride_duration\") / 60.0) \\\n",
        "    .withColumn(\"expected_fare\",\n",
        "        when(col(\"vehicle_type\") == \"ECONOMY\", 2.5 + 1.2 * col(\"distance_km\") + 0.3 * col(\"duration_min\"))\n",
        "        .when(col(\"vehicle_type\") == \"LUXURY\", 5.0 + 2.0 * col(\"distance_km\") + 0.6 * col(\"duration_min\"))\n",
        "        .when(col(\"vehicle_type\") == \"SUV\", 4.0 + 1.5 * col(\"distance_km\") + 0.4 * col(\"duration_min\"))\n",
        "    ) \\\n",
        "    .withColumn(\"relative_error\", (col(\"estimated_fare\") - col(\"expected_fare\")) / col(\"expected_fare\")) \\\n",
        "    .withColumn(\"is_anomaly\", col(\"relative_error\") > 0.3) \\\n",
        "    .withColumn(\"timestamp\", (col(\"timestamp\") / 1000).cast(\"timestamp\"))\n",
        "\n",
        "df_anomaly.createOrReplaceTempView(\"fare_anomalies_view\")"
      ],
      "metadata": {
        "id": "yOG1fVUcXSRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anomaly_count_query = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        'pricing_anomaly' AS metric_type,\n",
        "        vehicle_type AS dimension,\n",
        "        COUNT(*) AS metric_value,\n",
        "        window(timestamp, '5 minutes') AS window_interval\n",
        "    FROM fare_anomalies_view\n",
        "    WHERE is_anomaly = true\n",
        "    GROUP BY window(timestamp, '5 minutes'), vehicle_type\n",
        "\"\"\")\n",
        "\n",
        "anomaly_count_query.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"pricing_anomaly_sql\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiafU_zBXaTp",
        "outputId": "6c48dc91-1269-4b23-f33e-da31842bf099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.query.StreamingQuery at 0x7c0a5c26c910>"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM pricing_anomaly_sql\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBSzCOwDXewz",
        "outputId": "82baf710-6429-4bdb-aebd-632879ea3175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---------+------------+------------------------------------------+\n",
            "|metric_type    |dimension|metric_value|window_interval                           |\n",
            "+---------------+---------+------------+------------------------------------------+\n",
            "|pricing_anomaly|LUXURY   |2           |{2025-04-23 20:35:00, 2025-04-23 20:40:00}|\n",
            "|pricing_anomaly|ECONOMY  |14          |{2025-04-23 20:35:00, 2025-04-23 20:40:00}|\n",
            "+---------------+---------+------------+------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 15: Pricing analytics"
      ],
      "metadata": {
        "id": "W9j4wOMvYJMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_anomaly_area = df_anomaly.withColumn(\n",
        "    \"quadrant\",\n",
        "    when((col(\"pickup_latitude\") >= 40.75) & (col(\"pickup_longitude\") <= -73.95), \"NE\")\n",
        "    .when((col(\"pickup_latitude\") >= 40.75) & (col(\"pickup_longitude\") > -73.95), \"NW\")\n",
        "    .when((col(\"pickup_latitude\") < 40.75) & (col(\"pickup_longitude\") <= -73.95), \"SE\")\n",
        "    .otherwise(\"SW\")\n",
        ")\n",
        "\n",
        "df_anomaly_area.createOrReplaceTempView(\"fare_anomalies_area_view\")"
      ],
      "metadata": {
        "id": "EAmAmmVKZJM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anomaly_area_query = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        'pricing_anomaly_area' AS metric_type,\n",
        "        quadrant AS dimension,\n",
        "        COUNT(*) AS metric_value,\n",
        "        window(timestamp, '5 minutes') AS window_interval\n",
        "    FROM fare_anomalies_area_view\n",
        "    WHERE is_anomaly = true\n",
        "    GROUP BY window(timestamp, '5 minutes'), quadrant\n",
        "\"\"\")\n",
        "\n",
        "anomaly_area_query.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"pricing_anomaly_area_sql\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "koVYASQDZWn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM pricing_anomaly_area_sql\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJPv_MMOZYYD",
        "outputId": "21be152a-898d-4f1d-d8c8-aacbed75ed87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---------+------------+------------------------------------------+\n",
            "|metric_type         |dimension|metric_value|window_interval                           |\n",
            "+--------------------+---------+------------+------------------------------------------+\n",
            "|pricing_anomaly_area|NW       |21          |{2025-04-23 20:45:00, 2025-04-23 20:50:00}|\n",
            "|pricing_anomaly_area|SW       |1           |{2025-04-23 20:40:00, 2025-04-23 20:45:00}|\n",
            "|pricing_anomaly_area|SE       |35          |{2025-04-23 20:45:00, 2025-04-23 20:50:00}|\n",
            "|pricing_anomaly_area|SW       |21          |{2025-04-23 20:45:00, 2025-04-23 20:50:00}|\n",
            "|pricing_anomaly_area|NE       |30          |{2025-04-23 20:45:00, 2025-04-23 20:50:00}|\n",
            "|pricing_anomaly_area|NE       |2           |{2025-04-23 20:40:00, 2025-04-23 20:45:00}|\n",
            "+--------------------+---------+------------+------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 16: Driver Fairness Index (high-level)"
      ],
      "metadata": {
        "id": "9aoE_HzQZtJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Driver Fairness Index (DFI) per area will show how fairly rides and revenue are distributed among drivers of the same vehicle type and quadrant"
      ],
      "metadata": {
        "id": "-xYvWSIoa31p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, from_unixtime, when, window\n",
        "\n",
        "df_driver_fairness = df_passenger \\\n",
        "    .filter(col(\"status\") == \"COMPLETED\") \\\n",
        "    .withColumn(\"timestamp\", (col(\"timestamp\") / 1000).cast(\"timestamp\")) \\\n",
        "    .withColumn(\"quadrant\",\n",
        "        when((col(\"pickup_latitude\") >= 40.75) & (col(\"pickup_longitude\") <= -73.95), \"NE\")\n",
        "        .when((col(\"pickup_latitude\") >= 40.75) & (col(\"pickup_longitude\") > -73.95), \"NW\")\n",
        "        .when((col(\"pickup_latitude\") < 40.75) & (col(\"pickup_longitude\") <= -73.95), \"SE\")\n",
        "        .otherwise(\"SW\")\n",
        "    )\n",
        "\n",
        "df_driver_fairness.createOrReplaceTempView(\"driver_fairness_view\")"
      ],
      "metadata": {
        "id": "MJRHMJjZdO3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ride_counts = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        driver_id,\n",
        "        vehicle_type,\n",
        "        quadrant,\n",
        "        window(timestamp, '15 minutes') AS window_interval,\n",
        "        COUNT(*) AS ride_count\n",
        "    FROM driver_fairness_view\n",
        "    GROUP BY driver_id, vehicle_type, quadrant, window(timestamp, '15 minutes')\n",
        "\"\"\")\n",
        "\n",
        "ride_counts.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"ride_counts_sql\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esiC2Y7kdQy1",
        "outputId": "4a47a257-0ed2-4201-f285-d7ddd1b906af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.query.StreamingQuery at 0x7c0a66e93350>"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "earnings = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        driver_id,\n",
        "        vehicle_type,\n",
        "        quadrant,\n",
        "        window(timestamp, '15 minutes') AS window_interval,\n",
        "        SUM(estimated_fare) AS earnings\n",
        "    FROM driver_fairness_view\n",
        "    GROUP BY driver_id, vehicle_type, quadrant, window(timestamp, '15 minutes')\n",
        "\"\"\")\n",
        "\n",
        "earnings.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"earnings_sql\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlLP0DlmdTa6",
        "outputId": "5ce4975d-29ec-4d6b-8e47-524fe2bbe503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.query.StreamingQuery at 0x7c0a5c3ad9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ride_df = spark.sql(\"SELECT * FROM ride_counts_sql\")\n",
        "earn_df = spark.sql(\"SELECT * FROM earnings_sql\")\n",
        "\n",
        "driver_stats = ride_df.join(earn_df, on=[\"driver_id\", \"vehicle_type\", \"quadrant\", \"window_interval\"])\n",
        "driver_stats.createOrReplaceTempView(\"driver_stats_view\")"
      ],
      "metadata": {
        "id": "4iz--PnOdWeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fairness_index = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        vehicle_type,\n",
        "        quadrant,\n",
        "        window_interval,\n",
        "        STDDEV(ride_count) / AVG(ride_count) AS ride_fairness_index,\n",
        "        STDDEV(earnings) / AVG(earnings) AS earnings_fairness_index\n",
        "    FROM driver_stats_view\n",
        "    GROUP BY vehicle_type, quadrant, window_interval\n",
        "\"\"\")\n",
        "\n",
        "fairness_index.show(truncate=False)"
      ],
      "metadata": {
        "id": "kUbmQjjAdYTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 17: Unprofitable drivers"
      ],
      "metadata": {
        "id": "WZpwfA4lgh1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, from_unixtime, window, sum as sum_\n",
        "\n",
        "# 1. Convert timestamps correctly\n",
        "df_passenger_ts = df_passenger.withColumn(\n",
        "    \"timestamp_ts\", from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ")\n",
        "\n",
        "# Optional: register a temp view (for SQL use)\n",
        "df_passenger_ts.createOrReplaceTempView(\"passenger_events_view\")\n",
        "\n",
        "# 2. Filter completed trips, apply sliding window, and aggregate estimated fare\n",
        "unprofitable_driver_df = df_passenger_ts \\\n",
        "    .filter(col(\"status\") == \"COMPLETED\") \\\n",
        "    .withWatermark(\"timestamp_ts\", \"15 minutes\") \\\n",
        "    .groupBy(\n",
        "        window(col(\"timestamp_ts\"), \"5 minutes\", \"2 minutes\"),\n",
        "        col(\"driver_id\")\n",
        "    ) \\\n",
        "    .agg(sum_(\"estimated_fare\").alias(\"total_fare\"))\n",
        "\n",
        "# 3. Write results to memory table for querying\n",
        "query = unprofitable_driver_df \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"unprofitable_drivers_sql\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "s8dzIrPDjKDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT COUNT(*) FROM unprofitable_drivers_sql\").show()"
      ],
      "metadata": {
        "id": "5gJEMaUwm6Vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top5 = spark.sql(\"\"\"\n",
        "    SELECT *\n",
        "    FROM unprofitable_drivers_sql\n",
        "    ORDER BY total_fare ASC\n",
        "    LIMIT 5\n",
        "\"\"\").collect()\n",
        "\n",
        "for row in top5:\n",
        "    print(row)"
      ],
      "metadata": {
        "id": "0dtJpYk5l4_W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}