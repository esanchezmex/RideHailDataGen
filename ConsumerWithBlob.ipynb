{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dM4EyO1KSxAN",
        "outputId": "283a8d12-2aa5-4781-dd19-132fb9b0d560"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastavro\n",
            "  Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting confluent-kafka\n",
            "  Downloading confluent_kafka-2.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (22 kB)\n",
            "Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading confluent_kafka-2.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fastavro, confluent-kafka\n",
            "Successfully installed confluent-kafka-2.10.0 fastavro-1.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install fastavro confluent-kafka"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0MvrQPoen0y"
      },
      "source": [
        "# Spark Setup\n",
        "\n",
        "Reference: https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WLghayLdt_dm",
        "outputId": "3b3ec4b2-cbe0-4a95-c0ab-3be4597fb8a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'spark-3.5.5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Fetch the latest Spark 3.x.x version\n",
        "# curl -s https://downloads.apache.org/spark/ → Fetches the Spark download page.\n",
        "# grep -o 'spark-3\\.[0-9]\\+\\.[0-9]\\+' → Extracts only versions that start with spark-3. (ignoring Spark 4.x if it exists in the future).\n",
        "\n",
        "# sort -V → Sorts the versions numerically.\n",
        "# tail -1 → Selects the latest version.\n",
        "spark_version = subprocess.run(\n",
        "    \"curl -s https://downloads.apache.org/spark/ | grep -o 'spark-3\\\\.[0-9]\\\\+\\\\.[0-9]\\\\+' | sort -V | tail -1\",\n",
        "    shell=True, capture_output=True, text=True\n",
        ").stdout.strip()\n",
        "\n",
        "spark_version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yUGK1pfcfdQd"
      },
      "outputs": [],
      "source": [
        "spark_release=spark_version\n",
        "hadoop_version='hadoop3'\n",
        "\n",
        "import os, time\n",
        "start=time.time()\n",
        "os.environ['SPARK_RELEASE']=spark_release\n",
        "os.environ['HADOOP_VERSION']=hadoop_version\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_release}-bin-{hadoop_version}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Hm38TuFfdyk",
        "outputId": "6c3befe6-e975-49ef-f936-28b7ef412ef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.5\n"
          ]
        }
      ],
      "source": [
        "# Run below commands in google colab\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # install Java8\n",
        "!wget -q http://apache.osuosl.org/spark/${SPARK_RELEASE}/${SPARK_RELEASE}-bin-${HADOOP_VERSION}.tgz # download spark-3.3.X\n",
        "!tar xf ${SPARK_RELEASE}-bin-${HADOOP_VERSION}.tgz # unzip it\n",
        "\n",
        "!pip install -q findspark # install findspark\n",
        "# findspark find your Spark Distribution and sets necessary environment variables\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Check the pyspark version\n",
        "import pyspark\n",
        "print(pyspark.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKK0qqktP3ya"
      },
      "source": [
        "# Define the configuration details for your Spark job:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0Arkhzxg1aZ"
      },
      "source": [
        "Create your Spark session. You must define details of the Kafka Cluster to connect to, topic name and consumer group name.\n",
        "\n",
        "- kafka_brokers: List of Kafka bootstrap servers  \n",
        "- topic_name: The Kafka topic to read messages from\n",
        "- consumer_group: This allows you to use different Spark jobs to consume the same topic messages and implement different analytics\n",
        "- schema: the AVRO schema of topic messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oMRPA_3412_M"
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "event_hub_namespace = os.environ.get(\"event_hub_namespace\")\n",
        "\n",
        "passengers_eventhub_name=os.environ.get(\"passengers_eventhub_name\")\n",
        "passengers_conn_str=os.environ.get(\"passengers_conn_str\")\n",
        "\n",
        "drivers_eventhub_name=os.environ.get(\"drivers_eventhub_name\")\n",
        "drivers_conn_str=os.environ.get(\"drivers_conn_str\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TBTCdSx3g-Ud"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.avro.functions import from_avro\n",
        "\n",
        "# Define the schema (from github)\n",
        "with open(\"passengerschemav2.json\") as f:\n",
        "    pass_schema = f.read()\n",
        "\n",
        "with open(\"driver_schema.json\") as e:\n",
        "    drv_schema = e.read()\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"StreamingAVROFromKafka\") \\\n",
        "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-avro_2.12:3.5.0') \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TnuEZtgTgqVG"
      },
      "outputs": [],
      "source": [
        "kafkaConf_pass = {\n",
        "    \"kafka.bootstrap.servers\": f\"{event_hub_namespace}.servicebus.windows.net:9093\",\n",
        "    # Below settins required if kafka is secured, for example when connecting to Azure Event Hubs:\n",
        "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
        "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
        "    \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{passengers_conn_str}\";',\n",
        "\n",
        "    \"subscribe\": passengers_eventhub_name,\n",
        "    \"startingOffsets\": \"latest\", # \"latest\", \"earliest\", (by choosing earliest, you will consume all the data on the event hub immediately)\n",
        "        # by choosing \"latest\", you will consume only newly arriving data.\n",
        "\n",
        "\n",
        "\n",
        "    \"enable.auto.commit\": \"true \",\n",
        "    \"groupIdPrefix\": \"debug_specials_\",\n",
        "    \"auto.commit.interval.ms\": \"5000\"\n",
        "}\n",
        "\n",
        "kafkaConf_drv = {\n",
        "    \"kafka.bootstrap.servers\": f\"{event_hub_namespace}.servicebus.windows.net:9093\",\n",
        "    # Below settins required if kafka is secured, for example when connecting to Azure Event Hubs:\n",
        "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
        "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
        "    \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{drivers_conn_str}\";',\n",
        "\n",
        "    \"subscribe\": drivers_eventhub_name,\n",
        "    \"startingOffsets\": \"latest\", # \"latest\", \"earliest\", (by choosing earliest, you will consume all the data on the event hub immediately)\n",
        "        # by choosing \"latest\", you will consume only newly arriving data.\n",
        "\n",
        "\n",
        "\n",
        "    \"enable.auto.commit\": \"true \",\n",
        "    \"groupIdPrefix\": \"debug_specials_\",\n",
        "    \"auto.commit.interval.ms\": \"5000\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qvQ_eIvGTsQe"
      },
      "outputs": [],
      "source": [
        "# Read from Event Hub using Kafka\n",
        "df_rides = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .options(**kafkaConf_pass) \\\n",
        "    .load()\n",
        "\n",
        "# Deserialize the AVRO messages from the value column\n",
        "df_passenger = df_rides.select(from_avro(df_rides.value, pass_schema, {\"mode\": \"PERMISSIVE\"}).alias(\"passenger_events\"))\n",
        "\n",
        "# Read from Event Hub using Kafka\n",
        "df_driver = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .options(**kafkaConf_drv) \\\n",
        "    .load()\n",
        "\n",
        "# Deserialize the AVRO messages from the value column\n",
        "df_driver = df_driver.select(from_avro(df_driver.value, drv_schema, {\"mode\": \"PERMISSIVE\"}).alias(\"driver_event\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "MzrNKPETLLAg"
      },
      "outputs": [],
      "source": [
        "# Flatten the schemas\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_passenger = df_passenger.select(\n",
        "    col(\"passenger_events.request_id\"),\n",
        "    col(\"passenger_events.passenger_id\"),\n",
        "    col(\"passenger_events.timestamp\"),\n",
        "    col(\"passenger_events.pickup_location.latitude\").alias(\"pickup_latitude\"),\n",
        "    col(\"passenger_events.pickup_location.longitude\").alias(\"pickup_longitude\"),\n",
        "    col(\"passenger_events.dropoff_location.latitude\").alias(\"dropoff_latitude\"),\n",
        "    col(\"passenger_events.dropoff_location.longitude\").alias(\"dropoff_longitude\"),\n",
        "    col(\"passenger_events.vehicle_type\"),\n",
        "    col(\"passenger_events.passenger_preferences.music\").alias(\"music_preference\"),\n",
        "    col(\"passenger_events.passenger_preferences.temperature\").alias(\"preferred_temperature\"),\n",
        "    col(\"passenger_events.passenger_preferences.quiet_ride\").alias(\"quiet_ride\"),\n",
        "    col(\"passenger_events.payment_info.payment_method\").alias(\"payment_method\"),\n",
        "    col(\"passenger_events.payment_info.coupon_codes\").alias(\"coupon_codes\"),\n",
        "    col(\"passenger_events.payment_info.loyalty_points_used\").alias(\"loyalty_points_used\"),\n",
        "    col(\"passenger_events.estimated_fare\"),\n",
        "    col(\"passenger_events.text_messages\"),\n",
        "    col(\"passenger_events.driver_rating\"),\n",
        "    col(\"passenger_events.status\"),\n",
        "    col(\"passenger_events.driver_id\"),\n",
        "    col(\"passenger_events.request_timestamp\"),\n",
        "    col(\"passenger_events.accepted_timestamp\"),\n",
        "    col(\"passenger_events.ride_duration\")\n",
        ")\n",
        "\n",
        "df_driver = df_driver.select(\n",
        "      col(\"driver_event.driver_id\"),\n",
        "      col(\"driver_event.timestamp\"),\n",
        "      col(\"driver_event.latitude\"),\n",
        "      col(\"driver_event.longitude\"),\n",
        "      col(\"driver_event.status\")\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTING NEW QUERIES WITH BLOB"
      ],
      "metadata": {
        "id": "So6VROtUClJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install azure-storage-blob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUx-PIx8Iucz",
        "outputId": "c57cdf4c-5ed7-4c8a-f9c7-53c510b8ffe7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting azure-storage-blob\n",
            "  Downloading azure_storage_blob-12.25.1-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting azure-core>=1.30.0 (from azure-storage-blob)\n",
            "  Downloading azure_core-1.33.0-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.11/dist-packages (from azure-storage-blob) (43.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from azure-storage-blob) (4.13.2)\n",
            "Collecting isodate>=0.6.1 (from azure-storage-blob)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.30.0->azure-storage-blob) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.30.0->azure-storage-blob) (1.17.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.1.4->azure-storage-blob) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (2025.1.31)\n",
            "Downloading azure_storage_blob-12.25.1-py3-none-any.whl (406 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.0/407.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_core-1.33.0-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.1/207.1 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: isodate, azure-core, azure-storage-blob\n",
            "Successfully installed azure-core-1.33.0 azure-storage-blob-12.25.1 isodate-0.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Blob\n",
        "account_name = os.environ.get(\"account_name\")\n",
        "container_name = os.environ.get(\"container_name\")\n",
        "account_key = os.environ.get(\"account_key\")"
      ],
      "metadata": {
        "id": "JIySpRgcDHRb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "\n",
        "def process_batch(batch_df, batch_id, query_name):\n",
        "    \"\"\"Process each micro-batch and upload to Azure Blob\"\"\"\n",
        "    print(f\"Starting to process batch {batch_id} for {query_name}\")\n",
        "\n",
        "    try:\n",
        "        # Check if DataFrame is empty without calling isEmpty()\n",
        "        # This avoids the job cancellation issue\n",
        "        count = batch_df.count()\n",
        "        if count == 0:\n",
        "            print(f\"Batch {batch_id} for {query_name} is empty, skipping\")\n",
        "            return\n",
        "\n",
        "        # Create a unique directory for this batch\n",
        "        timestamp = int(time.time())\n",
        "        local_output_path = f\"/tmp/stream-output/{query_name}/{timestamp}_{batch_id}\"\n",
        "\n",
        "        # Explicitly create the directory\n",
        "        import os\n",
        "        os.makedirs(os.path.dirname(local_output_path), exist_ok=True)\n",
        "\n",
        "        # First write batch to local storage as Parquet with minimized partitions\n",
        "        # Use a try-except to handle potential job cancellations\n",
        "        try:\n",
        "            batch_df.coalesce(1).write.mode(\"overwrite\").parquet(local_output_path)\n",
        "        except Exception as write_error:\n",
        "            print(f\"Error writing batch {batch_id} to Parquet: {str(write_error)}\")\n",
        "            return\n",
        "\n",
        "        # Set up Azure Blob client\n",
        "        conn_str = f\"DefaultEndpointsProtocol=https;AccountName={account_name};AccountKey={account_key};EndpointSuffix=core.windows.net\"\n",
        "        blob_service_client = BlobServiceClient.from_connection_string(conn_str)\n",
        "        container_client = blob_service_client.get_container_client(container_name)\n",
        "\n",
        "        # Upload each Parquet file to Azure Blob\n",
        "        files_uploaded = 0\n",
        "        for root, dirs, files in os.walk(local_output_path):\n",
        "            for file in files:\n",
        "                if file.endswith(\".parquet\"):\n",
        "                    local_file_path = os.path.join(root, file)\n",
        "                    blob_path = f\"stream-output/{query_name}/{timestamp}_{batch_id}/{file}\"\n",
        "\n",
        "                    with open(local_file_path, \"rb\") as data:\n",
        "                        container_client.upload_blob(name=blob_path, data=data, overwrite=True)\n",
        "                    files_uploaded += 1\n",
        "\n",
        "        print(f\"Batch {batch_id} for {query_name} uploaded to Azure Blob Storage ({files_uploaded} files)\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing batch {batch_id}: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "CauiFduhKyLO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /tmp/stream-checkpoints"
      ],
      "metadata": {
        "id": "tM17pn1XK8xa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jar_dependencies= \",\".join([\n",
        "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\",\n",
        "    \"org.apache.spark:spark-avro_2.12:3.5.0\",\n",
        "    \"org.apache.hadoop:hadoop-azure:3.3.1\",          # Hadoop Azure connector\n",
        "    \"com.microsoft.azure:azure-storage:8.6.6\"        # Azure Blob SDK dependency\n",
        "])"
      ],
      "metadata": {
        "id": "-EOmHX0tDSVa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spark = SparkSession.builder \\\n",
        "#     .appName(\"StreamingAVROToBlob\") \\\n",
        "#     .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
        "#     .config(\"spark.jars.packages\", jar_dependencies) \\\n",
        "#     .config(f\"fs.azure.account.key.{account_name}.blob.core.windows.net\", account_key) \\\n",
        "#     .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
        "#     .master(\"local[*]\") \\\n",
        "#     .getOrCreate()\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StreamingAVROToBlob\") \\\n",
        "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
        "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-avro_2.12:3.5.0\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "CPNFeFYzDZLX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For passengers data\n",
        "query_name = 'all_passengers_BLOB'\n",
        "query_passengers = df_passenger.writeStream \\\n",
        "    .foreachBatch(lambda df, id: process_batch(df, id, query_name)) \\\n",
        "    .outputMode(\"update\") \\\n",
        "    .trigger(processingTime='30 seconds') \\\n",
        "    .option(\"checkpointLocation\", f\"/tmp/stream-checkpoints/{query_name}\") \\\n",
        "    .start()\n",
        "\n",
        "\n",
        "# For drivers data\n",
        "query_name = 'all_drivers_BLOB'\n",
        "query_drivers = df_driver.writeStream \\\n",
        "    .foreachBatch(lambda df, id: process_batch(df, id, query_name)) \\\n",
        "    .outputMode(\"update\") \\\n",
        "    .trigger(processingTime='30 seconds') \\\n",
        "    .option(\"checkpointLocation\", f\"/tmp/stream-checkpoints/{query_name}\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "NJwvVQCRLCCu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_passengers.status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubrIYZp4Ll1K",
        "outputId": "5c9d43c3-7216-4c30-d5bc-7cc20dacaced"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'message': 'Processing new data',\n",
              " 'isDataAvailable': True,\n",
              " 'isTriggerActive': True}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_drivers.status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhQqx1cqLmnc",
        "outputId": "e7f88caa-3f86-4e74-bac8-fc2c5d1936e4"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'message': 'Processing new data',\n",
              " 'isDataAvailable': True,\n",
              " 'isTriggerActive': True}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.text(f\"/tmp/stream-checkpoints/all_drivers_BLOB\")\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lo06XbrDaTUE",
        "outputId": "09becc9c-5e68-45d9-9f53-2cb7b0b2f3c0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 9 for all_drivers_BLOB uploaded to Azure Blob Storage (1 files)\n",
            "Batch 9 for all_drivers_BLOB uploaded to Azure Blob Storage (1 files)\n",
            "+---------------------------------------------+\n",
            "|value                                        |\n",
            "+---------------------------------------------+\n",
            "|{\"id\":\"25fb07e5-3722-4b56-9c35-f8b23b2e87ae\"}|\n",
            "+---------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "im0BZXFzjgeS"
      },
      "source": [
        "# Analytical Queries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx18nWfXQVBm"
      },
      "source": [
        "Your Spark job and input messages are ready to be worked on. Now, you can apply any transformations required to answer business questions.\n",
        "\n",
        "IMPORTANT NOTE: if in config you chose \"startingOffsets\": \"latest\", then you must send data AFTER running df.writeStream...\n",
        "In other words, Spark will only start 'consuming' events after you run .writeStream, meaning that it will show up as empty if no new events have been sent after running .writeStream. (For this to be a real-time analytics case, it should be set to latest, so our stats update as new data comes in. For testing purposes, easier to set it to 'earliest' cause then you just send once and can work with that)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNJMd8DRCc9T"
      },
      "source": [
        "## Setup of Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VEkMmi234eGt"
      },
      "outputs": [],
      "source": [
        "!mkdir checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If offset:Latest, send new events after running this cell.\n",
        "query_name='all_passengers'\n",
        "query_passengers=df_passenger.writeStream \\\n",
        "    .outputMode(\"update\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(query_name) \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "q_B1yG4LmFEm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If offset:Latest, send new events after running this cell.\n",
        "query_name='all_drivers'\n",
        "query_drivers=df_driver.writeStream \\\n",
        "    .outputMode(\"update\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(query_name) \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "Eva8uy4ypRzz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('show tables').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6x93DNtkpgI0",
        "outputId": "8c5ddcd0-fd71-46a0-bc24-51baaa1e0949"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------+-----------+\n",
            "|namespace|     tableName|isTemporary|\n",
            "+---------+--------------+-----------+\n",
            "|         |   all_drivers|       true|\n",
            "|         |all_passengers|       true|\n",
            "+---------+--------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Tm86HWwUVw1",
        "outputId": "7809562d-a75c-48b6-eea6-dfecdda3c36b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'message': 'Processing new data',\n",
              " 'isDataAvailable': True,\n",
              " 'isTriggerActive': True}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Status either \"Processing new data\" or \"Getting offsets from...\"\n",
        "query_passengers.status"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_drivers.status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAg7r3CVpn93",
        "outputId": "3a2c0022-3d77-49e4-df9d-cafa82f0f75d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'message': 'Processing new data',\n",
              " 'isDataAvailable': True,\n",
              " 'isTriggerActive': True}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark.sql(f'SELECT count(*) as record_count FROM all_drivers').show(20, truncate=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZ-kpix2kw1O",
        "outputId": "20789507-057e-4824-b045-b91d2556212c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|record_count|\n",
            "+------------+\n",
            "|         300|\n",
            "+------------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark.sql(f'SELECT count(*) as record_count FROM all_passengers').show(20, truncate=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gVCG9rsq0Ty",
        "outputId": "df312b8b-bd1d-4802-a0af-b20ef630e47c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|record_count|\n",
            "+------------+\n",
            "|          12|\n",
            "+------------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TubCWjP0Dlb",
        "outputId": "b41b06e8-5a6f-4de7-b4b6-78ab54afb660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+------------+-------------+------------------+------------------+------------------+------------------+------------+----------------+---------------------+----------+--------------+------------+-------------------+--------------+--------------------+-------------+---------+----------+-----------------+------------------+-------------+\n",
            "|         request_id|passenger_id|    timestamp|   pickup_latitude|  pickup_longitude|  dropoff_latitude| dropoff_longitude|vehicle_type|music_preference|preferred_temperature|quiet_ride|payment_method|coupon_codes|loyalty_points_used|estimated_fare|       text_messages|driver_rating|   status| driver_id|request_timestamp|accepted_timestamp|ride_duration|\n",
            "+-------------------+------------+-------------+------------------+------------------+------------------+------------------+------------+----------------+---------------------+----------+--------------+------------+-------------------+--------------+--------------------+-------------+---------+----------+-----------------+------------------+-------------+\n",
            "|REQ-1745435026-6836|      P00546|1745435026159| 40.65073501851697|-74.10117698180463|40.883232273433634|-73.94966756785688|     ECONOMY|         HIP_HOP|                   18|      true|        PAYPAL|          []|               NULL|     45.668888|                  []|         NULL|COMPLETED|driver_107|    1745435026159|     1745435491163|    3453.5112|\n",
            "|REQ-1745435027-8956|      P00532|1745435027164| 40.69852493219883|-74.12890879442683|  40.8460700340002|-73.85345648953884|     ECONOMY|         HIP_HOP|                   20|      true|     APPLE_PAY|          []|                 84|     45.042225|                  []|          1.7|COMPLETED|driver_136|    1745435027164|     1745435195442|    3403.3782|\n",
            "|REQ-1745435054-5030|      P00079|1745435054227| 40.69350650278326|-73.92070715771763|40.635778800304834|-73.94512733614388|     ECONOMY|             POP|                   21|     false|     APPLE_PAY|    [SAVE33]|               NULL|     12.594366|                  []|          4.7|COMPLETED|driver_225|    1745435054227|     1745435342924|    807.54926|\n",
            "|REQ-1745435020-5138|      P00228|1745435020118|40.890487158672286|-74.09742974069526| 40.68293695226072|-73.85967836222545|      LUXURY|       CLASSICAL|                   22|     false|        PAYPAL|          []|               NULL|      48.24408|                  []|         NULL|COMPLETED|driver_146|    1745435020118|     1745435926586|    3659.5264|\n",
            "|REQ-1745435060-2820|      P00351|1745435060268|40.889434425212556| -74.1299019089232| 40.89300521410469|-74.07151622388696|     ECONOMY|            JAZZ|                   21|      true|          CASH|          []|               NULL|     17.551647|                  []|         NULL|COMPLETED|driver_172|    1745435060268|     1745435622484|     589.8241|\n",
            "|REQ-1745435052-7741|      P00187|1745435052216| 40.66395587922572|-73.87245221985528|   40.751252419783|-73.98693289140124|      LUXURY|       CLASSICAL|                   20|     false|   CREDIT_CARD|          []|               NULL|     22.994905|                  []|          4.6|COMPLETED|driver_262|    1745435052216|     1745435405478|    1639.5924|\n",
            "|REQ-1745435031-5572|      P00180|1745435031189| 40.88681844744739| -73.9758147439598|40.661197085687405|-74.08892114637109|     ECONOMY|            JAZZ|                   20|      true|     APPLE_PAY|    [SAVE27]|               NULL|     42.681973|                  []|         NULL|COMPLETED|driver_282|    1745435031189|     1745435546603|    3214.5576|\n",
            "|REQ-1745435031-2342|      P00563|1745435031189|40.845822685660984|-73.88923938239445|40.602277618915075|-73.97983449761446|     ECONOMY|             POP|                   18|     false|          CASH|          []|               NULL|     44.630814|                  []|          2.4|COMPLETED|driver_163|    1745435031189|     1745435741972|     3370.465|\n",
            "|REQ-1745435062-1870|      P00431|1745435062281| 40.70801899122692|-74.05788073870832| 40.70689294727613|-73.97686603491427|     ECONOMY|            ROCK|                   23|      true|     APPLE_PAY|          []|               NULL|     12.727009|                  []|         NULL|COMPLETED|driver_005|    1745435062281|     1745435296480|     818.1607|\n",
            "|REQ-1745435065-9667|      P00577|1745435065302| 40.85232398668503|-73.90701936000828|40.846509960640965|-73.90911005550826|     ECONOMY|   NO_PREFERENCE|                   18|     false|     APPLE_PAY|          []|               NULL|     3.5032082|                  []|         NULL|COMPLETED|driver_080|    1745435065302|     1745435454158|     80.25666|\n",
            "|REQ-1745435057-5236|      P00588|1745435057243| 40.88785151998804| -73.8764061607629| 40.85260505278129| -73.9486997729501|     ECONOMY|            JAZZ|                   18|     false|    GOOGLE_PAY|    [SAVE45]|               NULL|     37.027927|                  []|         NULL|COMPLETED|driver_082|    1745435057243|     1745435631798|     866.4043|\n",
            "|REQ-1745435063-5808|      P00481|1745435063291| 40.61744777040271|-74.09914650306136|40.664037025944566|-74.08366838640367|     ECONOMY|            JAZZ|                   25|      true|    GOOGLE_PAY|          []|               NULL|     18.666319|                  []|         NULL|CANCELLED|driver_109|    1745435063291|     1745435311438|    293.56995|\n",
            "|REQ-1745435061-5454|      P00061|1745435061275|40.743462913647704|-74.07743076671922| 40.66365880449071|-74.08397452116452|     ECONOMY|   NO_PREFERENCE|                   19|      true|   CREDIT_CARD|          []|               NULL|     28.112055|[{SYS-1745435061-...|         NULL|COMPLETED|driver_076|    1745435061275|     1745435524594|    1065.0425|\n",
            "|REQ-1745435060-5891|      P00447|1745435060269|  40.8255243443482| -73.8401264419107|40.852061841633386|-73.90210776385821|     ECONOMY|            ROCK|                   23|     false|        PAYPAL|          []|                 69|     11.471106|                  []|         NULL|COMPLETED|driver_180|    1745435060269|     1745436080767|     717.6885|\n",
            "|REQ-1745435062-7110|      P00441|1745435062282| 40.79700957803462|-73.92976458593148|  40.8476488797053|-73.84658352805822|     ECONOMY|   NO_PREFERENCE|                   19|     false|          CASH|    [SAVE15]|               NULL|     15.951094|                  []|          2.8|COMPLETED|driver_201|    1745435062282|     1745435687087|    1076.0875|\n",
            "|REQ-1745435067-2399|      P00565|1745435067318| 40.74597298022453|-73.99207067609733| 40.73749880938508|-74.01643676795508|     ECONOMY|         HIP_HOP|                   23|     false|   CREDIT_CARD|          []|               NULL|      5.882152|                  []|         NULL|COMPLETED|driver_203|    1745435067318|     1745435750976|    270.57217|\n",
            "|REQ-1745435056-7702|      P00313|1745435056239| 40.60113598297778|-74.09332143143448| 40.64688680349182|-73.92987328953197|     ECONOMY|            ROCK|                   21|      true|     APPLE_PAY|          []|               NULL|     24.515337|                  []|          2.1|COMPLETED|driver_111|    1745435056239|     1745435546544|    1761.2269|\n",
            "|REQ-1745435064-5384|      P00412|1745435064298|  40.6363762088331|-74.10584430689715| 40.70824727798412|-74.09834028772136|     ECONOMY|   NO_PREFERENCE|                   19|      true|          CASH|    [SAVE29]|               NULL|     14.503995|                  []|          1.6|COMPLETED|driver_296|    1745435064298|     1745435750893|     960.3196|\n",
            "+-------------------+------------+-------------+------------------+------------------+------------------+------------------+------------+----------------+---------------------+----------+--------------+------------+-------------------+--------------+--------------------+-------------+---------+----------+-----------------+------------------+-------------+\n",
            "\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(spark.sql(f'SELECT * FROM all_passengers').show(20, truncate=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark.sql(f'SELECT * FROM all_drivers').show(20, truncate=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdD95QSoogr0",
        "outputId": "a23ad299-3033-43d3-e2f1-13dafa543b01"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+------------------+------------------+---------+\n",
            "| driver_id|    timestamp|          latitude|         longitude|   status|\n",
            "+----------+-------------+------------------+------------------+---------+\n",
            "|driver_000|1745435062276|40.725089908091384|-74.02561436848974|  ON_RIDE|\n",
            "|driver_001|1745435062278| 40.72652353684726|-73.95025208036424|  OFFLINE|\n",
            "|driver_002|1745435062278| 40.67645920013038| -74.0434774049815|  OFFLINE|\n",
            "|driver_003|1745435062278|40.879669765723904|-74.07841783926281|  OFFLINE|\n",
            "|driver_004|1745435062278| 40.76182214782082|-73.87144994657183|  OFFLINE|\n",
            "|driver_005|1745435062278| 40.72061256119842|-74.04169309515052|AVAILABLE|\n",
            "|driver_006|1745435062278| 40.78862051110513| -74.0016071273915|  OFFLINE|\n",
            "|driver_007|1745435062278|  40.6011054271701|-74.07100029862085|  OFFLINE|\n",
            "|driver_008|1745435062278|40.661197085687405|-74.08892114637109|  OFFLINE|\n",
            "|driver_009|1745435062278| 40.89448723897804|  -74.000468707782|  OFFLINE|\n",
            "|driver_010|1745435062278| 40.64571990296338|-73.85426034232256|  OFFLINE|\n",
            "|driver_011|1745435062278|40.754368010474934|-73.92650690886668|  OFFLINE|\n",
            "|driver_012|1745435062278| 40.83647369386344|-73.98465053287084|  OFFLINE|\n",
            "|driver_013|1745435062278| 40.69586474943386|-73.89971402525362|  OFFLINE|\n",
            "|driver_014|1745435062278|   40.774382809342|-74.08673794832272|  ON_RIDE|\n",
            "|driver_015|1745435062278|40.699821905632795|-74.07421019513154|  OFFLINE|\n",
            "|driver_016|1745435062278| 40.77129617851296|-74.04030052297583|  OFFLINE|\n",
            "|driver_017|1745435062278| 40.89872353898007|-73.88028616705904|  OFFLINE|\n",
            "|driver_018|1745435062278| 40.88020177599314|-74.00608813548476|  OFFLINE|\n",
            "|driver_019|1745435062278| 40.68342131565475|-73.89520028184695|  OFFLINE|\n",
            "+----------+-------------+------------------+------------------+---------+\n",
            "only showing top 20 rows\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write df_passenger to CSV\n",
        "query_passengers_to_csv = df_passenger.writeStream \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"path\", \"/tmp/passengers_stream_output\") \\\n",
        "    .option(\"checkpointLocation\", \"/tmp/passengers_checkpoint\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .start()\n",
        "\n",
        "# Write df_driver to CSV\n",
        "query_drivers_to_csv = df_driver.writeStream \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"path\", \"/tmp/drivers_stream_output\") \\\n",
        "    .option(\"checkpointLocation\", \"/tmp/drivers_checkpoint\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .start()\n",
        "\n",
        "time.sleep(35) # PRODUCER SHOULD BE SEIDNING NOW!!!\n",
        "\n",
        "query_passengers_to_csv.stop()\n",
        "query_drivers_to_csv.stop()\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "\n",
        "# Collect from memory sink and drop unsupported columns\n",
        "drivers_df = spark.sql(\"SELECT * FROM all_drivers\")\n",
        "passengers_df = spark.sql(\"SELECT * FROM all_passengers\").drop(\"coupon_codes\", \"text_messages\")\n",
        "\n",
        "# Coalesce to 1 partition and write with header\n",
        "drivers_df.coalesce(1).write.option(\"header\", True).mode(\"overwrite\").csv(\"/tmp/final_drivers_single\")\n",
        "passengers_df.coalesce(1).write.option(\"header\", True).mode(\"overwrite\").csv(\"/tmp/final_passengers_single\")\n",
        "\n",
        "# Rename single part file for drivers\n",
        "driver_csv = glob.glob(\"/tmp/final_drivers_single/part-*.csv\")[0]\n",
        "os.rename(driver_csv, \"/tmp/final_drivers_single/drivers.csv\")\n",
        "\n",
        "# Rename single part file for passengers\n",
        "passenger_csv = glob.glob(\"/tmp/final_passengers_single/part-*.csv\")[0]\n",
        "os.rename(passenger_csv, \"/tmp/final_passengers_single/passengers.csv\")\n",
        "\n",
        "# Zip folders\n",
        "shutil.make_archive(\"/tmp/final_drivers_csv_single\", 'zip', \"/tmp/final_drivers_single\")\n",
        "shutil.make_archive(\"/tmp/final_passengers_csv_single\", 'zip', \"/tmp/final_passengers_single\")\n",
        "\n",
        "# Move for download\n",
        "!cp /tmp/final_drivers_csv_single.zip /content/final_drivers_csv_single.zip\n",
        "!cp /tmp/final_passengers_csv_single.zip /content/final_passengers_csv_single.zip"
      ],
      "metadata": {
        "id": "xHlZS6hl_yNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "idF97nmAcD1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 1: number of driver updates per status"
      ],
      "metadata": {
        "id": "gK0syWHFsEmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import from_unixtime, col, window\n",
        "\n",
        "df_driver_ts = df_driver.withColumn(\n",
        "    \"timestamp_ts\",\n",
        "    from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ")\n",
        "\n",
        "df_driver_ts.createOrReplaceTempView(\"driver_events_view\")"
      ],
      "metadata": {
        "id": "ILdwlR0eufa1"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ride_metrics_query = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        'ride_metrics' AS metric_type,\n",
        "        status AS dimension,\n",
        "        COUNT(*) AS metric_value,\n",
        "        window(timestamp_ts, '1 hour') AS window_interval\n",
        "    FROM driver_events_view\n",
        "    GROUP BY window(timestamp_ts, '1 hour'), status\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "FaZB_FTIt7UX"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_name = 'driver_status_metrics_BLOB'\n",
        "ride_metrics_stream = ride_metrics_query.writeStream \\\n",
        "    .foreachBatch(lambda df, id: process_batch(df, id, query_name)) \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .trigger(processingTime='30 seconds') \\\n",
        "    .option(\"checkpointLocation\", f\"/tmp/stream-checkpoints/{query_name}\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "ivokk23hu8S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ride_metrics_stream.status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QPsv9nxdSVk",
        "outputId": "b95e0321-6f4a-4174-a3bc-307722b84f32"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'message': 'Processing new data',\n",
              " 'isDataAvailable': True,\n",
              " 'isTriggerActive': True}"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 2: Trips status"
      ],
      "metadata": {
        "id": "CGXcBf1gDV_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import from_unixtime, col, window\n",
        "\n",
        "# Add timestamp column (if not already present)\n",
        "df_passenger_ts = df_passenger.withColumn(\n",
        "    \"timestamp_ts\",\n",
        "    from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ")\n",
        "\n",
        "# Create temporary view\n",
        "df_passenger_ts.createOrReplaceTempView(\"passenger_events_view\")\n",
        "\n",
        "# Define the windowed trip status aggregation\n",
        "trip_status_query = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        'trip_status' AS metric_type,\n",
        "        status AS dimension,\n",
        "        COUNT(*) AS metric_value,\n",
        "        window(timestamp_ts, '1 hour') AS window_interval\n",
        "    FROM passenger_events_view\n",
        "    GROUP BY window(timestamp_ts, '1 hour'), status\n",
        "\"\"\")\n",
        "\n",
        "# Stream to Azure Blob\n",
        "query_name = \"trip_status_metrics_BLOB\"\n",
        "trip_status_stream = trip_status_query.writeStream \\\n",
        "    .foreachBatch(lambda df, id: process_batch(df, id, query_name)) \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .trigger(processingTime=\"30 seconds\") \\\n",
        "    .option(\"checkpointLocation\", f\"/tmp/stream-checkpoints/{query_name}\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRtNgf1o80Ou",
        "outputId": "2a6aa697-a0b0-4174-c70d-73928964d14e"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing batch 25: An error occurred while calling o3632.count.\n",
            ": org.apache.spark.SparkException: Job 911 cancelled part of cancelled job group a3c3445a-dbd2-4770-b7ce-4b694caaa4c5\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2731)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleJobGroupCancelled$4(DAGScheduler.scala:1198)\n",
            "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
            "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:1197)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3016)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
            "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
            "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3616)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3615)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n",
            "\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3615)\n",
            "\tat sun.reflect.GeneratedMethodAccessor70.invoke(Unknown Source)\n",
            "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
            "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
            "\tat com.sun.proxy.$Proxy29.call(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-12-d03e644844f7>\", line 11, in process_batch\n",
            "    count = batch_df.count()\n",
            "            ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.5.5-bin-hadoop3/python/pyspark/sql/dataframe.py\", line 1240, in count\n",
            "    return int(self._jdf.count())\n",
            "               ^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.5.5-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    return_value = get_return_value(\n",
            "                   ^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.5.5-bin-hadoop3/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
            "    return f(*a, **kw)\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/content/spark-3.5.5-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
            "    raise Py4JJavaError(\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling o3632.count.\n",
            ": org.apache.spark.SparkException: Job 911 cancelled part of cancelled job group a3c3445a-dbd2-4770-b7ce-4b694caaa4c5\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2731)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleJobGroupCancelled$4(DAGScheduler.scala:1198)\n",
            "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
            "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:1197)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3016)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
            "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
            "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3616)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3615)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
            "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n",
            "\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3615)\n",
            "\tat sun.reflect.GeneratedMethodAccessor70.invoke(Unknown Source)\n",
            "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
            "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
            "\tat com.sun.proxy.$Proxy29.call(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trip_status_stream.status"
      ],
      "metadata": {
        "id": "vAIs8hCbn-Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 3: average response time per vehicle type"
      ],
      "metadata": {
        "id": "A-gfHUcSv30i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, from_unixtime, window, unix_timestamp\n",
        "\n",
        "df_passenger_ts = df_passenger.withColumn(\n",
        "    \"timestamp_ts\", from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"request_ts\", from_unixtime(col(\"request_timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"accepted_ts\", from_unixtime(col(\"accepted_timestamp\") / 1000).cast(\"timestamp\")\n",
        ")\n",
        "\n",
        "df_passenger_ts.createOrReplaceTempView(\"passenger_events_view\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-x0TLoFacFHR",
        "outputId": "32a896a3-55be-4ccf-d395-678ab0bd9e75"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1 for trip_status_metrics_BLOB uploaded to Azure Blob Storage (1 files)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_time_query = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        'response_time' AS metric_type,\n",
        "        vehicle_type AS dimension,\n",
        "        AVG((unix_timestamp(accepted_ts) - unix_timestamp(request_ts)) / 60.0) AS metric_value,\n",
        "        window(timestamp_ts, '15 minutes', '5 minutes').end AS window_end\n",
        "    FROM passenger_events_view\n",
        "    WHERE status = 'COMPLETED'\n",
        "    GROUP BY window(timestamp_ts, '15 minutes', '5 minutes'), vehicle_type\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHvXIZoicFHR",
        "outputId": "8d75419f-4da4-4fc5-d74a-063fca8a6177"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3 for trip_status_metrics_BLOB uploaded to Azure Blob Storage (1 files)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_name = 'response_time_BLOB'\n",
        "response_time_query = response_time_query.writeStream \\\n",
        "    .foreachBatch(lambda df, id: process_batch(df, id, query_name)) \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .trigger(processingTime='30 seconds') \\\n",
        "    .option(\"checkpointLocation\", f\"/tmp/stream-checkpoints/{query_name}\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "Z2ntB30-cFHR"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_time_query.status"
      ],
      "metadata": {
        "id": "0HQsQzJ2oD_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 4: Average Ride Duration per Hour of Day"
      ],
      "metadata": {
        "id": "sZ7z2hSkk-8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import from_unixtime, col, hour\n",
        "\n",
        "# Enrich timestamps if not already present\n",
        "df_passenger_ts = df_passenger.withColumn(\n",
        "    \"timestamp_ts\", from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"request_ts\", from_unixtime(col(\"request_timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"accepted_ts\", from_unixtime(col(\"accepted_timestamp\") / 1000).cast(\"timestamp\")\n",
        ")\n",
        "\n",
        "# Temporary view for SQL\n",
        "df_passenger_ts.createOrReplaceTempView(\"passenger_events_view\")\n",
        "\n",
        "# Query: Average ride duration per hour\n",
        "ride_duration_query = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        'ride_duration' AS metric_type,\n",
        "        CAST(HOUR(timestamp_ts) AS STRING) AS dimension,\n",
        "        AVG(ride_duration) AS metric_value,\n",
        "        window(timestamp_ts, '60 minutes', '60 minutes').end AS window_end\n",
        "    FROM passenger_events_view\n",
        "    WHERE status = 'COMPLETED'\n",
        "    GROUP BY window(timestamp_ts, '60 minutes', '60 minutes'), HOUR(timestamp_ts)\n",
        "\"\"\")\n",
        "\n",
        "# Write to blob using foreachBatch\n",
        "query_name = \"ride_duration_metrics_BLOB\"\n",
        "ride_duration_stream = ride_duration_query.writeStream \\\n",
        "    .foreachBatch(lambda df, id: process_batch(df, id, query_name)) \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .trigger(processingTime=\"30 seconds\") \\\n",
        "    .option(\"checkpointLocation\", f\"/tmp/stream-checkpoints/{query_name}\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3oUrvedlKzP",
        "outputId": "1e0b8688-668c-48fa-9a38-3ce1e7bfaa81"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to process batch 80 for ride_duration_metrics_BLOB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ride_duration_stream.status"
      ],
      "metadata": {
        "id": "puaDAPlPoGEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 5: requested/accepted ratio per area"
      ],
      "metadata": {
        "id": "SHAfBQ_XpPcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, from_unixtime, when\n",
        "\n",
        "df_passenger_quadrant = df_passenger.withColumn(\n",
        "    \"timestamp\", from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"quadrant\", when((col(\"pickup_latitude\") >= 40.75) & (col(\"pickup_longitude\") >= -73.95), \"NE\")\n",
        "                .when((col(\"pickup_latitude\") >= 40.75) & (col(\"pickup_longitude\") < -73.95), \"NW\")\n",
        "                .when((col(\"pickup_latitude\") < 40.75) & (col(\"pickup_longitude\") >= -73.95), \"SE\")\n",
        "                .otherwise(\"SW\")\n",
        ")\n",
        "\n",
        "df_passenger_quadrant.createOrReplaceTempView(\"passenger_quadrant_view\")"
      ],
      "metadata": {
        "id": "_Bx89eMoJE_n"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ride_ratio_query = spark.sql(\"\"\"\n",
        "SELECT\n",
        "  'request_acceptance_ratio' AS metric_type,\n",
        "  quadrant AS dimension,\n",
        "  SUM(CASE WHEN accepted_timestamp IS NOT NULL THEN 1 ELSE 0 END) * 1.0 /\n",
        "  COUNT(*) AS metric_value,\n",
        "  window(timestamp, '15 minutes', '5 minutes') AS window_interval\n",
        "FROM passenger_quadrant_view\n",
        "GROUP BY window(timestamp, '15 minutes', '5 minutes'), quadrant\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "_WN6IUNGJJBY"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_name = 'request_acceptance_ratio_BLOB'\n",
        "ride_ratio_query = ride_ratio_query.writeStream \\\n",
        "    .foreachBatch(lambda df, id: process_batch(df, id, query_name)) \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .trigger(processingTime='30 seconds') \\\n",
        "    .option(\"checkpointLocation\", f\"/tmp/stream-checkpoints/{query_name}\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "Dp-mMJUrJLrl"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ride_ratio_query.status"
      ],
      "metadata": {
        "id": "xL03Z5cMoIby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 6: Average Response Time per Vehicle Type"
      ],
      "metadata": {
        "id": "v-P8owFohnFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, from_unixtime\n",
        "\n",
        "# Enrich with usable timestamps\n",
        "df_passenger_ts = df_passenger.withColumn(\n",
        "    \"timestamp\", from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"request_ts\", from_unixtime(col(\"request_timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"accepted_ts\", from_unixtime(col(\"accepted_timestamp\") / 1000).cast(\"timestamp\")\n",
        ")\n",
        "\n",
        "# Create view\n",
        "df_passenger_ts.createOrReplaceTempView(\"passenger_events_view\")\n",
        "\n",
        "# Response time query with window and dimension\n",
        "response_time_query = spark.sql(\"\"\"\n",
        "SELECT\n",
        "  'response_time' AS metric_type,\n",
        "  vehicle_type AS dimension,\n",
        "  AVG((unix_timestamp(accepted_ts) - unix_timestamp(request_ts)) / 60.0) AS metric_value,\n",
        "  window(timestamp, '15 minutes', '5 minutes') AS window_interval\n",
        "FROM passenger_events_view\n",
        "WHERE status = 'COMPLETED'\n",
        "GROUP BY window(timestamp, '15 minutes', '5 minutes'), vehicle_type\n",
        "\"\"\")\n",
        "\n",
        "# Write to blob\n",
        "query_name = \"response_time_metrics_BLOB\"\n",
        "response_time_stream = response_time_query.writeStream \\\n",
        "    .foreachBatch(lambda df, id: process_batch(df, id, query_name)) \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .trigger(processingTime=\"30 seconds\") \\\n",
        "    .option(\"checkpointLocation\", f\"/tmp/stream-checkpoints/{query_name}\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "P_Xki5nelRhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_time_stream.status"
      ],
      "metadata": {
        "id": "8c3iXHBhoKGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 7: Active drivers per area"
      ],
      "metadata": {
        "id": "1mLaNSBFH66d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quadrants:\n",
        "\n",
        "NE: lat ≥ 40.75 and lon ≥ -73.95\n",
        "\n",
        "NW: lat ≥ 40.75 and lon < -73.95\n",
        "\n",
        "SE: lat < 40.75 and lon ≥ -73.95\n",
        "\n",
        "SW: lat < 40.75 and lon < -73.95\n",
        "\n"
      ],
      "metadata": {
        "id": "TRYCsFuXImlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "df_driver_quadrant = df_driver.withColumn(\n",
        "    \"timestamp\", from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"quadrant\", when((col(\"latitude\") >= 40.75) & (col(\"longitude\") >= -73.95), \"NE\")\n",
        "                .when((col(\"latitude\") >= 40.75) & (col(\"longitude\") < -73.95), \"NW\")\n",
        "                .when((col(\"latitude\") < 40.75) & (col(\"longitude\") >= -73.95), \"SE\")\n",
        "                .otherwise(\"SW\")\n",
        ")\n",
        "\n",
        "df_driver_quadrant.createOrReplaceTempView(\"driver_quadrant_view\")"
      ],
      "metadata": {
        "id": "SBi-d5L8Ing6"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "online_drivers_query = spark.sql(\"\"\"\n",
        "SELECT\n",
        "  'online_driver_count' AS metric_type,\n",
        "  quadrant AS dimension,\n",
        "  COUNT(*) AS metric_value,\n",
        "  window(timestamp, '15 minutes', '5 minutes') AS window_interval\n",
        "FROM driver_quadrant_view\n",
        "WHERE status IN ('AVAILABLE', 'ON_RIDE')\n",
        "GROUP BY window(timestamp, '15 minutes', '5 minutes'), quadrant\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "ueoH4L8sIqXy"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_name = 'online_driver_count_BLOB'\n",
        "online_drivers_query = online_drivers_query.writeStream \\\n",
        "    .foreachBatch(lambda df, id: process_batch(df, id, query_name)) \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .trigger(processingTime='30 seconds') \\\n",
        "    .option(\"checkpointLocation\", f\"/tmp/stream-checkpoints/{query_name}\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpC8K-gDIr0_",
        "outputId": "058c31a2-9762-46f8-f9ee-364af3260bf5"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error writing batch 25 to Parquet: An error occurred while calling o4329.parquet.\n",
            ": java.lang.InterruptedException\n",
            "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)\n",
            "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:980)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
            "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
            "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)\n",
            "\tat sun.reflect.GeneratedMethodAccessor76.invoke(Unknown Source)\n",
            "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
            "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
            "\tat com.sun.proxy.$Proxy29.call(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "online_drivers_query.status"
      ],
      "metadata": {
        "id": "6AFgo5E0oNXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 8: Average Wait Time per Area"
      ],
      "metadata": {
        "id": "7Gq4KthjhzbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, from_unixtime, when\n",
        "\n",
        "# Enrich DataFrame with timestamps and area quadrant\n",
        "df_wait_ts = df_passenger.withColumn(\n",
        "    \"timestamp\", from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"request_ts\", from_unixtime(col(\"request_timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"accepted_ts\", from_unixtime(col(\"accepted_timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"quadrant\", when((col(\"pickup_latitude\") >= 40.75) & (col(\"pickup_longitude\") >= -73.95), \"NE\")\n",
        "               .when((col(\"pickup_latitude\") >= 40.75) & (col(\"pickup_longitude\") < -73.95), \"NW\")\n",
        "               .when((col(\"pickup_latitude\") < 40.75) & (col(\"pickup_longitude\") >= -73.95), \"SE\")\n",
        "               .otherwise(\"SW\")\n",
        ")\n",
        "\n",
        "# Temp view\n",
        "df_wait_ts.createOrReplaceTempView(\"wait_time_quadrant_view\")\n",
        "\n",
        "# Query: wait time by quadrant and window\n",
        "wait_time_query = spark.sql(\"\"\"\n",
        "SELECT\n",
        "  'average_wait_time' AS metric_type,\n",
        "  quadrant AS dimension,\n",
        "  AVG(unix_timestamp(accepted_ts) - unix_timestamp(request_ts)) AS metric_value,\n",
        "  window(timestamp, '15 minutes', '5 minutes') AS window_interval\n",
        "FROM wait_time_quadrant_view\n",
        "WHERE accepted_ts IS NOT NULL AND request_ts IS NOT NULL\n",
        "GROUP BY window(timestamp, '15 minutes', '5 minutes'), quadrant\n",
        "\"\"\")\n",
        "\n",
        "# Stream to Azure\n",
        "query_name = \"wait_time_metrics_BLOB\"\n",
        "wait_time_stream = wait_time_query.writeStream \\\n",
        "    .foreachBatch(lambda df, id: process_batch(df, id, query_name)) \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .trigger(processingTime=\"30 seconds\") \\\n",
        "    .option(\"checkpointLocation\", f\"/tmp/stream-checkpoints/{query_name}\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO5r0WiklY_P",
        "outputId": "23c53f9d-cb58-41f7-e8f2-ae4900a19c46"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error writing batch 7 to Parquet: An error occurred while calling o4559.parquet.\n",
            ": java.lang.InterruptedException\n",
            "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)\n",
            "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:980)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
            "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
            "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)\n",
            "\tat sun.reflect.GeneratedMethodAccessor76.invoke(Unknown Source)\n",
            "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
            "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
            "\tat com.sun.proxy.$Proxy29.call(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wait_time_stream.status"
      ],
      "metadata": {
        "id": "W2zobGysoQnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 9: Driver Utilization Rate"
      ],
      "metadata": {
        "id": "xcTu-ALPL92K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_driver_ts = df_driver.withColumn(\n",
        "    \"timestamp\", from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ")\n",
        "\n",
        "df_driver_ts.createOrReplaceTempView(\"driver_events_view\")"
      ],
      "metadata": {
        "id": "FQ54uo38MJZz"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "utilization_query = spark.sql(\"\"\"\n",
        "SELECT\n",
        "  'driver_utilization' AS metric_type,\n",
        "  'global' AS dimension,\n",
        "  SUM(CASE WHEN status = 'ON_RIDE' THEN 1 ELSE 0 END) * 1.0 /\n",
        "  SUM(CASE WHEN status IN ('AVAILABLE', 'ON_RIDE') THEN 1 ELSE 0 END) AS metric_value,\n",
        "  window(timestamp, '15 minutes', '5 minutes') AS window_interval\n",
        "FROM driver_events_view\n",
        "WHERE status IN ('AVAILABLE', 'ON_RIDE')\n",
        "GROUP BY window(timestamp, '15 minutes', '5 minutes')\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "jtq48YfhMO8t"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_name = 'driver_utilization_BLOB'\n",
        "utilization_query = utilization_query.writeStream \\\n",
        "    .foreachBatch(lambda df, id: process_batch(df, id, query_name)) \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .trigger(processingTime='30 seconds') \\\n",
        "    .option(\"checkpointLocation\", f\"/tmp/stream-checkpoints/{query_name}\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "FEm84UTuMSzm"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "utilization_query.status"
      ],
      "metadata": {
        "id": "po9f2-jMoTTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 10: Vehicle Type Demand Share"
      ],
      "metadata": {
        "id": "I39Rt8muh8Nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, from_unixtime, window\n",
        "\n",
        "# Prepare timestamp\n",
        "df_passenger_ts = df_passenger.withColumn(\n",
        "    \"timestamp\", from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ")\n",
        "\n",
        "# Function to compute vehicle type demand share per batch\n",
        "def compute_vehicle_share(batch_df, epoch_id):\n",
        "    vehicle_counts = batch_df.filter(col(\"status\") == \"COMPLETED\") \\\n",
        "        .groupBy(\n",
        "            window(col(\"timestamp\"), \"15 minutes\", \"5 minutes\"),\n",
        "            col(\"vehicle_type\")\n",
        "        ) \\\n",
        "        .count() \\\n",
        "        .withColumnRenamed(\"count\", \"vehicle_count\")\n",
        "\n",
        "    total_counts = batch_df.filter(col(\"status\") == \"COMPLETED\") \\\n",
        "        .groupBy(\n",
        "            window(col(\"timestamp\"), \"15 minutes\", \"5 minutes\")\n",
        "        ) \\\n",
        "        .count() \\\n",
        "        .withColumnRenamed(\"count\", \"total_count\")\n",
        "\n",
        "    joined = vehicle_counts.join(\n",
        "        total_counts,\n",
        "        on=\"window\"\n",
        "    ).withColumn(\n",
        "        \"metric_type\", col(\"vehicle_type\") * 0 + 1  # Dummy to include field\n",
        "    ).selectExpr(\n",
        "        \"'vehicle_type_share' AS metric_type\",\n",
        "        \"vehicle_type AS dimension\",\n",
        "        \"window AS window_interval\",\n",
        "        \"vehicle_count * 1.0 / total_count AS metric_value\"\n",
        "    )\n",
        "\n",
        "    process_batch(joined, epoch_id, \"vehicle_type_share_BLOB\")\n",
        "\n",
        "# WriteStream using foreachBatch\n",
        "df_passenger_ts.writeStream \\\n",
        "    .foreachBatch(compute_vehicle_share) \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .trigger(processingTime=\"30 seconds\") \\\n",
        "    .option(\"checkpointLocation\", \"/tmp/stream-checkpoints/vehicle_type_share_BLOB\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTtPn2PUlf-R",
        "outputId": "eeed5a60-fd0e-42d1-8c77-b8334676c54d"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.query.StreamingQuery at 0x785bc2949850>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_passenger_ts.status"
      ],
      "metadata": {
        "id": "CCN7sUe2oVPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 11: Cancellation Rate"
      ],
      "metadata": {
        "id": "PjZghRIyNzC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cancellation_query = spark.sql(\"\"\"\n",
        "SELECT\n",
        "  'cancellation_rate' AS metric_type,\n",
        "  'global' AS dimension,\n",
        "  window(timestamp_ts, '15 minutes', '5 minutes') AS window_interval,\n",
        "  SUM(CASE WHEN status = 'CANCELLED' THEN 1 ELSE 0 END) * 1.0 /\n",
        "  COUNT(*) AS metric_value\n",
        "FROM passenger_events_view\n",
        "GROUP BY window(timestamp_ts, '15 minutes', '5 minutes')\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "query_name = 'cancellation_rate_BLOB'\n",
        "cancellation_stream = cancellation_query.writeStream \\\n",
        "    .foreachBatch(lambda df, id: process_batch(df, id, query_name)) \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .trigger(processingTime='30 seconds') \\\n",
        "    .option(\"checkpointLocation\", f\"/tmp/stream-checkpoints/{query_name}\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "qTVyPHF2OX7o"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cancellation_stream.status"
      ],
      "metadata": {
        "id": "yMcBkSB3oXpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 12: Ride Matching Delay by Area"
      ],
      "metadata": {
        "id": "evL1YQ6miFpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, from_unixtime, when\n",
        "\n",
        "# Enrich with timestamps and quadrant\n",
        "df_match_ts = df_passenger.withColumn(\n",
        "    \"timestamp\", from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"request_ts\", from_unixtime(col(\"request_timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"accepted_ts\", from_unixtime(col(\"accepted_timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"quadrant\", when((col(\"pickup_latitude\") >= 40.75) & (col(\"pickup_longitude\") >= -73.95), \"NE\")\n",
        "               .when((col(\"pickup_latitude\") >= 40.75) & (col(\"pickup_longitude\") < -73.95), \"NW\")\n",
        "               .when((col(\"pickup_latitude\") < 40.75) & (col(\"pickup_longitude\") >= -73.95), \"SE\")\n",
        "               .otherwise(\"SW\")\n",
        ")\n",
        "\n",
        "# Temp view\n",
        "df_match_ts.createOrReplaceTempView(\"matching_delay_view\")\n",
        "\n",
        "# SQL query for match delay by quadrant\n",
        "match_delay_query = spark.sql(\"\"\"\n",
        "SELECT\n",
        "  'match_delay' AS metric_type,\n",
        "  quadrant AS dimension,\n",
        "  AVG((unix_timestamp(accepted_ts) - unix_timestamp(request_ts)) / 60.0) AS metric_value,\n",
        "  window(timestamp, '15 minutes', '5 minutes') AS window_interval\n",
        "FROM matching_delay_view\n",
        "WHERE accepted_ts IS NOT NULL AND request_ts IS NOT NULL\n",
        "GROUP BY window(timestamp, '15 minutes', '5 minutes'), quadrant\n",
        "\"\"\")\n",
        "\n",
        "# Stream to Azure\n",
        "query_name = \"match_delay_metrics_BLOB\"\n",
        "match_delay_stream = match_delay_query.writeStream \\\n",
        "    .foreachBatch(lambda df, id: process_batch(df, id, query_name)) \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .trigger(processingTime=\"30 seconds\") \\\n",
        "    .option(\"checkpointLocation\", f\"/tmp/stream-checkpoints/{query_name}\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "G7EcDwSqlmj4"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "match_delay_stream.status"
      ],
      "metadata": {
        "id": "FZAAiSunoZd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 13: Drive Efficiency (high-level)"
      ],
      "metadata": {
        "id": "kiWiCQhlRCtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Measures the distance and sees if drivers are being efficient**"
      ],
      "metadata": {
        "id": "d5NZRRn_TulF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Efficiency Ratio= Ride Duration / Straight-line Distance\n",
        "​"
      ],
      "metadata": {
        "id": "vrgVVpWrRFmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We assume Haversine distance (straight-line) is “ideal”\n",
        "\n",
        "Duration = ride_duration in seconds"
      ],
      "metadata": {
        "id": "dtNVKJZXRssW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, radians, sin, cos, atan2, sqrt, lit\n",
        "\n",
        "R = 6371.0  # Earth radius in kilometers\n",
        "\n",
        "df_efficiency = df_passenger.withColumn(\n",
        "    \"pickup_lat_rad\", radians(col(\"pickup_latitude\"))\n",
        ").withColumn(\n",
        "    \"pickup_lon_rad\", radians(col(\"pickup_longitude\"))\n",
        ").withColumn(\n",
        "    \"dropoff_lat_rad\", radians(col(\"dropoff_latitude\"))\n",
        ").withColumn(\n",
        "    \"dropoff_lon_rad\", radians(col(\"dropoff_longitude\"))\n",
        ").withColumn(\n",
        "    \"dlat\", col(\"dropoff_lat_rad\") - col(\"pickup_lat_rad\")\n",
        ").withColumn(\n",
        "    \"dlon\", col(\"dropoff_lon_rad\") - col(\"pickup_lon_rad\")\n",
        ").withColumn(\n",
        "    \"a\", sin(col(\"dlat\") / 2) ** 2 + cos(col(\"pickup_lat_rad\")) * cos(col(\"dropoff_lat_rad\")) * sin(col(\"dlon\") / 2) ** 2\n",
        ").withColumn(\n",
        "    \"c\", 2 * atan2(sqrt(col(\"a\")), sqrt(1 - col(\"a\")))\n",
        ").withColumn(\n",
        "    \"distance_km\", R * col(\"c\")\n",
        ").withColumn(\n",
        "    \"timestamp\", from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ")\n",
        "\n",
        "df_efficiency.createOrReplaceTempView(\"route_efficiency_view\")"
      ],
      "metadata": {
        "id": "UG8lpHS0RlJ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8b0c917-bb09-4e00-a474-983f569f5a3f"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to process batch 45 for cancellation_rate_BLOB\n",
            "Starting to process batch 9 for cancellation_rate_BLOB\n",
            "Starting to process batch 7 for cancellation_rate_BLOB\n",
            "Starting to process batch 4 for cancellation_rate_BLOB\n",
            "Starting to process batch 10 for cancellation_rate_BLOB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "efficiency_query = spark.sql(\"\"\"\n",
        "SELECT\n",
        "  'route_efficiency' AS metric_type,\n",
        "  vehicle_type AS dimension,\n",
        "  AVG(distance_km / (ride_duration / 60.0)) AS metric_value,  -- km per minute\n",
        "  window(timestamp, '15 minutes', '5 minutes') AS window_interval\n",
        "FROM route_efficiency_view\n",
        "WHERE ride_duration > 0 AND distance_km IS NOT NULL\n",
        "GROUP BY window(timestamp, '15 minutes', '5 minutes'), vehicle_type\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "egxD3XP8SPLE"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_name = 'route_efficiency_BLOB'\n",
        "efficiency_query = efficiency_query.writeStream \\\n",
        "    .foreachBatch(lambda df, id: process_batch(df, id, query_name)) \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .trigger(processingTime='30 seconds') \\\n",
        "    .option(\"checkpointLocation\", f\"/tmp/stream-checkpoints/{query_name}\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "0l9eKSsWSRv6"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "efficiency_query.status"
      ],
      "metadata": {
        "id": "uBbBpaGXocAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 14: Pricing Analytics Anomalies (High Level)"
      ],
      "metadata": {
        "id": "Bq6Dr5jfiSu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, radians, sin, cos, atan2, sqrt, lit, when, unix_timestamp, window\n",
        "\n",
        "R = 6371.0  # Earth radius in km\n",
        "\n",
        "# Compute anomaly flags and required fields\n",
        "df_anomaly = df_passenger.withColumn(\"pickup_lat_rad\", radians(col(\"pickup_latitude\"))) \\\n",
        "    .withColumn(\"pickup_lon_rad\", radians(col(\"pickup_longitude\"))) \\\n",
        "    .withColumn(\"dropoff_lat_rad\", radians(col(\"dropoff_latitude\"))) \\\n",
        "    .withColumn(\"dropoff_lon_rad\", radians(col(\"dropoff_longitude\"))) \\\n",
        "    .withColumn(\"dlat\", col(\"dropoff_lat_rad\") - col(\"pickup_lat_rad\")) \\\n",
        "    .withColumn(\"dlon\", col(\"dropoff_lon_rad\") - col(\"pickup_lon_rad\")) \\\n",
        "    .withColumn(\"a\", sin(col(\"dlat\") / 2) ** 2 + cos(col(\"pickup_lat_rad\")) * cos(col(\"dropoff_lat_rad\")) * sin(col(\"dlon\") / 2) ** 2) \\\n",
        "    .withColumn(\"c\", 2 * atan2(sqrt(col(\"a\")), sqrt(1 - col(\"a\")))) \\\n",
        "    .withColumn(\"distance_km\", R * col(\"c\")) \\\n",
        "    .withColumn(\"duration_min\", col(\"ride_duration\") / 60.0) \\\n",
        "    .withColumn(\"expected_fare\",\n",
        "        when(col(\"vehicle_type\") == \"ECONOMY\", 2.5 + 1.2 * col(\"distance_km\") + 0.3 * col(\"duration_min\"))\n",
        "        .when(col(\"vehicle_type\") == \"LUXURY\", 5.0 + 2.0 * col(\"distance_km\") + 0.6 * col(\"duration_min\"))\n",
        "        .when(col(\"vehicle_type\") == \"SUV\", 4.0 + 1.5 * col(\"distance_km\") + 0.4 * col(\"duration_min\"))\n",
        "    ) \\\n",
        "    .withColumn(\"relative_error\", (col(\"estimated_fare\") - col(\"expected_fare\")) / col(\"expected_fare\")) \\\n",
        "    .withColumn(\"is_anomaly\", col(\"relative_error\") > 0.3) \\\n",
        "    .withColumn(\"timestamp\", (col(\"timestamp\") / 1000).cast(\"timestamp\"))\n",
        "\n",
        "# View for SQL\n",
        "df_anomaly.createOrReplaceTempView(\"fare_anomalies_view\")\n",
        "\n",
        "# Query: count anomalies per vehicle type per 5-minute window\n",
        "anomaly_count_query = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        'pricing_anomaly' AS metric_type,\n",
        "        vehicle_type AS dimension,\n",
        "        COUNT(*) AS metric_value,\n",
        "        window(timestamp, '5 minutes') AS window_interval\n",
        "    FROM fare_anomalies_view\n",
        "    WHERE is_anomaly = true\n",
        "    GROUP BY window(timestamp, '5 minutes'), vehicle_type\n",
        "\"\"\")\n",
        "\n",
        "# Write to blob\n",
        "query_name = \"pricing_anomaly_metrics_BLOB\"\n",
        "anomaly_stream = anomaly_count_query.writeStream \\\n",
        "    .foreachBatch(lambda df, id: process_batch(df, id, query_name)) \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .trigger(processingTime=\"30 seconds\") \\\n",
        "    .option(\"checkpointLocation\", f\"/tmp/stream-checkpoints/{query_name}\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahOxbuKNl61E",
        "outputId": "02058591-ab4a-4316-b711-14a8814697d4"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error writing batch 7 to Parquet: An error occurred while calling o4771.parquet.\n",
            ": java.lang.InterruptedException\n",
            "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)\n",
            "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:980)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
            "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
            "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)\n",
            "\tat sun.reflect.GeneratedMethodAccessor76.invoke(Unknown Source)\n",
            "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
            "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
            "\tat com.sun.proxy.$Proxy29.call(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "anomaly_stream.status"
      ],
      "metadata": {
        "id": "aZ56RjpwoffD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation 15: Pricing analytics"
      ],
      "metadata": {
        "id": "W9j4wOMvYJMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure to convert timestamp to timestamp_ts first if not already done\n",
        "df_anomaly_area = df_anomaly.withColumn(\n",
        "    \"timestamp_ts\",\n",
        "    from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")\n",
        ").withColumn(\n",
        "    \"quadrant\",\n",
        "    when((col(\"pickup_latitude\") >= 40.75) & (col(\"pickup_longitude\") <= -73.95), \"NE\")\n",
        "    .when((col(\"pickup_latitude\") >= 40.75) & (col(\"pickup_longitude\") > -73.95), \"NW\")\n",
        "    .when((col(\"pickup_latitude\") < 40.75) & (col(\"pickup_longitude\") <= -73.95), \"SE\")\n",
        "    .otherwise(\"SW\")\n",
        ")\n",
        "\n",
        "df_anomaly_area.createOrReplaceTempView(\"fare_anomalies_area_view\")\n",
        "\n",
        "# Fix the query to use timestamp_ts instead of timestamp\n",
        "anomaly_area_query = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        'pricing_anomaly_area' AS metric_type,\n",
        "        quadrant AS dimension,\n",
        "        COUNT(*) AS metric_value,\n",
        "        window(timestamp_ts, '5 minutes') AS window_interval\n",
        "    FROM fare_anomalies_area_view\n",
        "    WHERE is_anomaly = true\n",
        "    GROUP BY window(timestamp_ts, '5 minutes'), quadrant\n",
        "\"\"\")\n",
        "\n",
        "# Add blob storage integration\n",
        "query_name = 'pricing_anomaly_area_BLOB'\n",
        "anomaly_area_stream = anomaly_area_query.writeStream \\\n",
        "    .foreachBatch(lambda df, id: process_batch(df, id, query_name)) \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .trigger(processingTime='30 seconds') \\\n",
        "    .option(\"checkpointLocation\", f\"/tmp/stream-checkpoints/{query_name}\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "koVYASQDZWn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anomaly_area_stream.status"
      ],
      "metadata": {
        "id": "EQtCE7f1oic7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXDP4mRF1Ngt"
      },
      "source": [
        "# Stop your queries and your spark job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EZ5N_SnhqDk"
      },
      "outputs": [],
      "source": [
        "# Set to True and run cell when you want to stop your queries and Spark job.\n",
        "if True:\n",
        "  # Get the list of active streaming queries\n",
        "  active_queries = spark.streams.active\n",
        "\n",
        "# Print details about each active query\n",
        "  for query in active_queries:\n",
        "      query.stop()\n",
        "      print(f\"Query Name: {query.name}\")\n",
        "      print(f\"Query ID: {query.id}\")\n",
        "      print(f\"Query Status: {query.status}\")\n",
        "      print(f\"Is Query Active: {query.isActive}\")\n",
        "      print(\"-\" * 50)\n",
        "  spark.stop()\n",
        "  spark.sparkContext.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}